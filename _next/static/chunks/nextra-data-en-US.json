{"/coming_soon":{"title":"Coming Soon","data":{}},"/":{"title":"resyfer's Dev Blog","data":{"":"Hi, I am Saurav Pal, and I code under the name resyfer. Here's a blog to share my two cents on stuff.","about-me#About Me":"I'm an enthusiastic developer who's open to learning anything and everything related to operating systems, embedded systems, and networking, and I like open source.I am a Google Summer of Code 2024 (GSoC '24') contributor to Apache organization in which I am working on designing and implementing mnemofs, which is a filesystem for NAND Flashes for the Apache NuttX Real Time Operating System (RTOS).I'm also currently an upcoming Associate Software Engineer at Oracle.I was also a GSoC '23 contributor to PostgreSQL as well, working on improving PostgreSQL version support for pgexporter.I am currently a fresh graduate from the National Institute of Technology, Silchar, which is one of the Institutions of National Importance in India, in which I was pursing a Bachelor of Technology (B. Tech.) in Computer Science and Engineering (CSE) for 4 years (from Nov 2020 - May 2024).","blogs#Blogs":"Here are my blogs.\nmnemofs"}},"/mnemofs/ba":{"title":"Mnemofs's Block Allocator","data":{"":"The block allocator of mnemofs is inspired by littlefs a lot, but it differs a lot. Since NAND flashes are a common device used for sensors, the allocation of blocks or pages needs to be fast. This means shifting the RAM vs. speed tradeoff a little bit so that it consumes more RAM.The block allocator wants to be fair and spread wear as evenly as possible. So, like littlefs's block allocator, this starts with a random block and keeps assigning pages. Now, in mnemofs, pages and blocks can both be expected from the block allocator. In this case, the block allocator skips pages until the start of the next nearest block, and allocates from there. Since most block allocations are done by internal data structures, and they happen in bulk, skipped pages are not too much in count. Contiguous allocation may happen, but it may not as well.The block allocator contains a bitmap of each page to denote if it's being used or not. The block allocator also contains an array of counters for each block in the device. These counters count the number of pages inside that block that want to be deleted. If all of the pages inside a block want to be erased, then the next time the block allocator comes across this block (or any page in it), it will immediately erase the block and then allocate blocks or pages.","conclusion#Conclusion":"So, all in all, these are the ins and outs of mnemofs. Is it a weird place to end the series? There are follow-up blogs of my journey in designing and implementing this file system!"}},"/mnemofs/endeval":{"title":"Google Summer of Code '24 @ Apache NuttX Final Report","data":{"":"TL;DR: NO! No shortcuts, it's worth it I promise.\nHi, I am Saurav Pal. I was a Google Summer of Code (GSoC) '24 contributor to the Apache NuttX Real Time Operating System (RTOS), working on designing and implementing mnemofs, a file system for NAND flashes, aiming at enabling file system support for the NAND flashes in the Apache NuttX RTOS.I'll be taking you through my entire work. Prerequisites you ask? None. Actually, you just need to know English. I'll be explaining everything else.","storage-devices#Storage devices":"A lot of storage devices are available like floppy disks. Ok, probably not the best example as some readers may not even have seen floppy disks in their lives. Compact Disks (CDs), Hard Disk Drives (HDDs), and Solid State Drives (SDDs) are some more popular and slightly more recent mediums of storage. NAND flashes are one such storage device.When we talk about storage devices, we mean non-volatile memory. This means that the stored data is persistent even when power is no longer being provided to the storage device. Random Access Memory (RAM) is the standard example of a volatile memory medium, which loses all data once it loses power. Some common storage devices for embedded systems include: EPROM, EEPROM, NOR Flash and NAND Flash. NAND flashes and NOR flashes are very close competitiors, and are very often selected to be used on embedded system projects when someone doesn't need too much storage.","nand-vs-nor#NAND vs NOR":"Now, what are NAND gates or NOR gates?\nGood question, because it's something you're not required to know for this report.\nWhat are NAND or NOR flashes?\nGood question, because it's something you're required to know for this report.\nNAND and NOR flashes are made out of NAND and NOR gates (like mentioned above, not required to know). Formally, NAND flashes are a type of programmable, non-volatile storage technology built with cells of NAND gates.Why, where, and which should I use? It's complicated. NAND flashes require less number of lines in their structure and thus are more compact and cheaper than NOR flashes as they require less silicon for the same storage space. Sequential reads are faster in NAND flash, but NAND flashes are slower in random access reads. NOR flashes allow a single machine word to be written to an erased location or read independently. NAND flashes require reads or writes to be in pages, and erases to be in blocks (more on this later). Moreover, NAND flashes are more prone to random bit flipping, and a higher percentage of blocks turn out to be bad blocks right from manufacture, compared to NOR (again, further explanation below).","structure-of-nand-flashes#Structure of NAND flashes":"The structure of NAND flashes is very multi-layered for others but simple for us. All we need to know is that a NAND flash is made up of tiny units called blocks, which are themselves made out of tinier units called pages, which further are made up of even tinier units called cells.While we don't have to think down to the cell level, it's good to know some things about it. In a Single-Level Cell (SLC) NAND flash, each cell is one bit. There are other levels like Multi-Level Cell (MLC) NAND flashes and Triple-Level Cell (TLC) NAND flashes depending on the number of bits per cell.What we do need to consider are pages and blocks. A block is the smallest erasable unit, while a page is the smallest readable or writeable unit. What is the role of erasable you might ask? Well, if a page was previously written to, it needs to be erased before it can be written again. But since a block is the smallest erasable unit, you'll need to erase the entire block to update a certain page in the block. Another troublesome problem with the erase operation is that on every erase, a block is slightly more worn out than before. Upon repeated erase, it might not be able to reliably store data anymore, which means that it's not guaranteed that the data you read from any page within the block is the data that was written to that page.The onset of this state of the unreliability of a block means that the block is now a \"bad block\". However, usage is not the sole cause of bad blocks in NAND flashes. Any good manufacturer tests their products before selling them. In the process of testing a NAND flash, some blocks might be deemed to be unfit for storage right from manufacture itself as they exhibit the unreliability of bad blocks, and are thus also clubbed under bad blocks.Mentioned above is the tendency for NAND flashes to undergo random bit flipping. What is bit flipping? As the name suggests, the bits might flip sides (pretty treacherous, I know). This is a phenomenon not specific to NAND flashes, and over the years, people have come up with various solutions to deal with this. The solution NAND flashes use is to have some redundant bits to recognize and rectify bit flips, called Error Correction Code (ECC) bits. These parity bits may be stored on the page itself, but a common consensus has come up to keep these bits in a separate area on the page.Thus, a page is split into two parts: a spare area, and a user data area. The user data area contains, well, the user data. The spare area contains the bits for ECC, even some bit(s) for bad block marking (called a bad block marker), and some other bits. A bad block marker is mostly used by the manufacturer to denote whether a particular block is a bad block.","virtual-file-system#Virtual File System":"What you should know is that Apache NuttX, like most Operating Systems (OSes), has a subsystem called the Virtual File System (VFS). This is technically not a file system itself, but it is tasked with the job of dealing with file systems. What you need to know is that the VFS mandates each file system, that aims to be used as part of an OS, to expose its functionality by following a fixed interface. This means that the file system needs to follow some rules regarding what it is expected to do and it needs to follow some expectations of behaviour. This situation can be fully understood using a hypothetical situation I just made up.Imagine the VFS doesn't exist. You ask your OS to help save the text \"hello world\" to a file with a name and location of your choice. Now, without any standards of communication or expectations that come with these standards, it might be the case that, depending on which file system you choose, their functionality might range from doing nothing to probably hacking the NSA. This is chaos. The OS can't decide which function to call from each FS for your desired task.To enable this, either the OS needs to know the names of the functions of each file system, or the file systems have to mention the names of the functions that correspond to each functionality that the OS expects a file system to have. We have evolved to agree on the latter, as it puts the burden onto the poor FS developers like me, and also allows you to use file systems that might not be officially included in the OS, but rather, built by some sketchy guy you found on the dark web which you find to be surprisingly the fastest (and which seems to be making a lot of network requests sometimes wink).Thus, this allows the side effects of a file system operations on the rest of the operating system to be uniform, defined, and predictable (on paper, at the very least) regardless of the file system being used. Thus, the rest of the OS, or even the user, does not need to care about the specifics of the underlying file system, and thus, a VFS enables an OS to be able to support multiple file systems at once. In short, a VFS mandates what the file system does, but not how they do it (this is where file systems differ from each other).Further, let's say, hypothetically, all FSes could hack the NSA and they each tell the OS the function that will start the hack as required by the VFS. Some day, a new FS is developed by a budding programmer who is not interested in these mundane and boring tasks. What the FS wants to do is launch some of the nuclear missiles that might be owned by your government. Now, this is unusual behaviour for a file system (in this hypothetical world). All file systems have stopped till the line of hacking the NSA. To enable this feature, the OS would need to expand its VFS. The latter part is what happens in the real world as well, though, I can not gaurantee about the former. Any new \"feature\" expected of file systems needs to be implemented by expanding the VFS. Also, if you can guess, after the expansion of the VFS, both FSes can co-exist. They both implement some features of the VFS, while ignoring others, which is perfectly fine.\nSince we're living in happier times, our file systems are content with just managing our storage devices. In fact, since we're talking about standards, POSIX has some of their own guidelines on what a file system is expected to do.","file-system-constructs#File System Constructs":"File systems ultimately help manage storing ones and zeroes onto storage devices to be painless for the user. They abstract away the peculiarities of the underlying electronic storage device, the pain of remembering the location of each bit of data, and many other such things. Over the years, file systems have evolved to agree on some basic concepts (with the help of FOMO of the POSIX and the big-shots OS party). Almost all of these are OS enforced, so, unless your FS wants to do weird gymnastics to circumvent this architecture, it would be great if your FS follows this.","files-and-directories#Files and Directories":"Some of these constructs are namely files and folders/directories, often collectively called FS objects. You group data belonging to one specific thing as one file, while you arrange multiple files or other directories corresponding to each topic into one directory. It's almost entirely dependent on where you draw the line.","root-and-path#Root and Path":"Another of these constructs is the root of a file system and path. Root is the name given to the directory that contains all other files or directories, and is created by all file systems before you even use it. As you can see, this entire directory and file relationship gives rise to a tree-like structure. Since a file can not have files inside it (that's the job of a directory), the leaf nodes of the tree might be either files or empty directories. Further, the internal nodes of the tree will be directories, with the root node of the tree being the root of the file system, hence the name.Let's say, the root has a directory inside it with the name \"hello\", and inside this directory, there is a file with the name \"world\". The path is a construct that allows us to relate an FS object with its location. So, the file's location, in this case, will be \"/hello/world\". The root has no name (empty string), and each level is separated by a /.","mount-points#Mount Points":"Yet another construct of FSes is mount points. It's a slightly complicated topic, so you need to read this part a bit slowly. So, you see, an FS works on a storage device. The storage device has to be formatted with the FS you want to use on it. Well, may be not ALL of the devices, if you consider partitions. However, for the purposes of this report, we assume you want to format the entire device.Your OS has its own preferred and internally supported file system that houses the root of the entire machine's file system. This FS is given the title of \"rootfs\". Rootfs is the title given to any FS that the OS picks to represent its entire FS tree's root (/). This root is the big guy, the supreme mugwump. In the entire FS tree, nothing comes above it.As it's a file system, it will undoubtedly have its own tree. An interesting thing to note is that it's not just the file systems that work with the VFS. VFS also works with device drivers, but it's slightly out of the scope of this report. Just assume that we can work with devices through their device drivers in the same way we would work with a file. In Unix, everything is a file.So, your storage driver works with VFS and is represented symbolically as a file in your device tree. If it's a file, surely it must have a path right? Well, it does. Usually, Unix-based OSes put the device's device file in the directory /dev. The path means there is a dev directory inside the root. This directory also belongs to the rootfs. Inside this will be a large number of files (and directories). Each file represents a device connected to your machine. Some of them may be physically present, and some might be logical devices (again, out of the scope of this report).Now, let's say, your NAND flash device connected to your machine is listed as the file /dev/nand, and you want to use mnemofs on it. Well, you can just mount it. What it means is that you tell the OS that this device is to be, or is already formatted with, mnemofs. Further, this also means you want to connect the file system that is present in this device to a preferred location in your rootfs.Just like your rootfs has a root, so does the file system that's present in your storage device. Root is nothing but the top-most guy. That may be globally true in the case of rootfs's root, or locally true, in the case of roots of FSes formatted in the storage devices.Mounting means attaching the local roots of the FSes in storage devices to locations in your rootfs tree. So, let's say, your local root has two directories a_local and b_local inside it. You mount your local root to the path /hi and provide the correct file system type. This means that these two directories can now be located by anyone or anything on your machine using the paths /hi/a_local and /hi/b_local respectively.Now, this might be confusing. You might think, if the device is already located at /dev/nand, why can't I do something like /dev/nand/a_local? Now, that does make sense, but in Unix, there's a distinction between device files and file systems.Firstly, device files are files and are the leaf nodes of the FS tree. They are not supposed to do the work of directories, and root nodes (local or global) are directories. Second, device files are an interface to the device driver of the storage device. You can use this driver to give it instructions on what to store where in its device. But, why should I do this? Doesn't this sound like the very thing file systems do? Yes. File systems, after being mounted, internally interact with your device's device driver (using this device file). What file systems do is arrange the logical constructs called files and directories that are stored inside your device at a convenient location in your rootfs upon mounting.The location where a file system is mounted is called a mount point. When it comes to mounting, we call the file system instance that was formatted onto the device the \"file system\", and the type of file system as \"file system type\". For example, in the NAND device mentioned above, /hi is the mount point and represents the file system (ie. the file system instance formatted onto the device), while mnemofs is the file system type.Do note that one FS type is not necessarily limited to one mount point. There can be multiple devices using the same FS type. As an FS developer, it is important to not have any \"global variables\", as each mount point is supposed to be independent of the other. All information needs to be instance-specific. How to do that? Well, patience. It will be explained later.","mnemofs#Mnemofs":"Mnemofs is a file system built specifically to support NAND flashes for embedded systems using the Apache NuttX RTOS. Its components are a superblock, a master node, a journal, an LRU, and a block allocator (to name a few). These work in layers of abstraction, so I'll walk you through them one by one.Due to the peculiarities of NAND flashes, we do not want to \"update\" a page. To update a page, we'll need to erase the entire block and write all of it (with the update) again. This will be very bad for block wear for multiple updates to the same block. Thus, we employ a tactic called Copy On Write (CoW). To update data in a file, we make a copy in memory, then update it, and then write the new information in a new place, and then update the location of this file.Now, in its pure manner, Copy On Write cascades up to the root. Why? So, a file is updated by writing it in a new location. And the new location needs to be updated in its parent. Which again triggers CoW, and so on till it goes on upward in the FS tree till it reaches the local root (it might go on even further, but that's dependent on the rootfs).For even one byte this goes up to the local root. A lot of writing for one byte right? Remember, the garbage left by CoW, ie. the pages that are old (and their data have been moved to another location), needs to be cleared when the page is to be used again. Mnemofs has measures to help mitigate this.Also, file update seems to take a lot of memory if we have to copy the entire thing in memory, make updates, and save it to a new location right? Especially for large files? Well, we have measures to mitigate this as well.","superblock#Superblock":"There are two versions of the superblock (SB). One that is formatted on the device, while the other that is present in memory. What is the difference? The former is pretty much useless for mnemofs, while mnemofs can't be used without the latter.The on-flash SB is just a magic sequence of 8 bytes and some information about the device and is stored in the very first good block in the storage. The only purpose it serves is to be a very quick reference on whether the device is formatted or not. Other than that it contains no information that can't be obtained or derived from the device driver of the storage device. The on-flash SB, once formatted, is not updated. It's there to stay. It's pretty much symbolic and does not bring anything to the table.The in-memory SB is stored in the private field of the driver and contains information about the device. Wait, but wasn't it the same as above? Yes. The difference is, here we need to query the information from the device only once, and keep it in memory as part of in-memory SB. From then, this stored information will be used instead of querying the device again and again (it's slower that way). It's useful in-memory, but it hardly serves any purpose if stored on the flash.","block-allocator#Block Allocator":"First, let's get the dragon in the room out of the way. The block allocator. It's a fairly separate component of the file system. Another component asks for a page or a block, and this allocates one for it and gives them the page or block number to it. Kind of like a hotel receptionist.Let's assume a hotel with all rooms identical. When a person is vacated, the room must be given a thorough cleaning before the next person is shifted in it. Also, assume the hotel cleaning staff will only thoroughly clean an entire floor at once, nothing less that than.Let's expand on the process of room allotment in such a hypothetical hotel. It's best to allocate everything serially and cycle back to the beginning once you reach the end and continue. By the time you cycle back to a place, it may have been vacated since the last time you visited it and will be ready to be occupied again. Why is this the best? Because it ensures fairness to all the rooms and ensures the maximum wear difference of 1 between rooms when they will be cleaned.Now, this can be applied to blocks and pages in our NAND flash. When pages are requested, the block allocator cycles through the available pages serially. It maintaines a bit mask to know which page is free or occupied by data.Now, imagine a large group of people came, and demanded that they want to book an entire floor. Now the perfect sequential iteration plan has been thrown out of the window right? Well, the receptionist will just skip some rooms, keep them empty, and then allot them the nearest floor available. Now, a problem occurs. Your previous plan assumed no unoccupied room would be cleaned. Why wear down a room when it does not even need to be cleaned? So you change the meaning of the first bit mask to mean dirty rooms. And you keep another bit mask to know which rooms can be cleaned.Once an entire floor is dirty and all the rooms on the floor can be cleaned, you call the cleaning crew. Until then, you only allocate pages that are cleaned. If a room is dirty and not ready to be cleaned, it means the resident is still occupying it. You can't just throw them out after all.This is basically how the block allocator works in mnemofs. In a cyclical sequential manner, it keeps allocating clean pages or blocks that do not have to be erased. There are some limitations on when a page marks itself as ready to be erased, and some caveats to the entire erase operation, but we'll look into them later.Suppose the receptionist dies one day, and a new guy takes over. This new guy doesn't know which room the receptionist had last allotted. So, if he starts allotting from the start, then the problem is that the rooms at the start will have more wear. Imagine if it's very common for receptionists to die. However unfortunate this incident will be, it will also pose a strong problem of wear misbalance in the hotel. Thus, each new receptionist will start from a random room instead of from the start.The block allocator in mnemofs also starts from a random page every time it's initialized. The receptionist dying can be correlated with power loss in embedded systems, ie. the death of the block allocator, as it stays purely in memory. Volatile memory remember?","vfs-methods#VFS Methods":"Mnemofs exposes its own functions corresponding to every VFS-mandated FS method. For example, the one corresponding to open(2) is the function mnemofs_open. Pretty intuitive right? Yeah. All sunshine in this section. Any VFS method is basically mnemofs_<method-name>.These VFS methods usually have some functionalities related to traversing the directories, creating new FS objects, removing FS objects, moving FS objects, getting stats of FS objects, updating FS objects, and reading FS objects.","rw-layer#R/W Layer":"This layer is very simple. You give a byte array 101101011010101110101110 (3 bytes) and tell it to store this on page 4, and it will either do it well, or return an error (either a bad block, or if the page is already written to, or if the device is not found, etc.), or it can also help you read the contents of a page.It can also mark any block you want as a bad block, and even check if a block is bad or not.Very simple stuff and this is the layer that works directly with the device driver of the storage device.","ctz-layer#CTZ Layer":"Mnemofs stores files and directories as Count Trailing Zero (CTZ) lists. These are a kind of backward skip list pioneered by littlefs, which follow a predictable pattern. Mnemofs doesn't follow this design word for word, but there are very minor changes, which do not bring any extra improvements to the table. CTZ lists are very good for updating, and especially appending, data to an existing file as governed by the as illustrated by the littlefs design specifications.So, I'll be explaining the mnemofs version (which may overlap with the littlefs version in most ways). CTZ skip lists are made out of CTZ blocks. Each CTZ block in mnemofs takes up the space of one single page. The naming is kept like this to preserve the original nomenclature. Each CTZ block has a \"data\" section, and a \"pointer\" section. CTZ lists have a storage overhead of 2 bytes on average per CTZ block (again, as derived by the littlefs team), and this means that on average, each page will have 8 bytes of pointers in mnemofs, which I think is the best deal I've heard in quite a while. Most pages are usually more than 512 B, which makes the overhead at a maximum of 1.5%.Formally, every CTZ block at index i (0-based indexing) has a pointer for every whole number x such that i - 2^x >= 0 and i is divisible by 2^x. Examples will explain it much better. A CTZ block with index 6 will have pointers to 5 (6 - 1) and 4 (6 - 2). A CTZ block with index 4 will have pointers to 3 (4 - 1), 2 (4 - 2) and 0 (4 - 4). Also, a CTZ block with index 5 will have a pointer to 4 (5 - 1). In mnemofs, the pointer to i - 2^x lies in the byte range [page_start + page_size - (4 * (x + 1)), page_start + page_size - (4 * x)). Each pointer takes up 4 bytes of space, and it's created as a bottom-up approach, where the pointer to the immediate neighbour is the last, and the furthest neighbour's pointer appears the first out of the pointers.So, apart from the area reserved for the pointers at the end of the page, the rest of the area at the front is for storing the data. This CTZ layer abstracts the data area and makes it seem like it's a single contiguous space for storage.For example, if the page size is 128 bytes, then the first block (index 0) has 128 bytes of data area, and the second and third blocks have 124 and 120 bytes of data area respectively and so on. So, if you had to store, say, 255 bytes of data, the layer just takes in the array you want to store, and the CTZ list, and this layer will split into (128 + 124 + 3) bytes. This means that the data area of the first two blocks will be full, while the third block (index 2) will have only 3 bytes at the front.This abstraction is also applicable for reading data from the CTZ list. Everything happens under the hood.Littlefs developers have derived a very convenient method of finding out the CTZ block index and CTZ block offset (ie. page offset in mnemofs) from the data offset (ie. offset into the data area, assuming it to be a contiguous space). They've also demonstrated how a CTZ list can be identified by just using the location (page number) of the last CTZ block and the index number of the last CTZ block.Over that CTZ lists are quite suitable for updates in CoW environments, as shown by the design document as well. The part before the update is kept the same, and the later part is updated and written to a new location.","files-and-directories-1#Files and Directories":"Files and directories are represented by CTZ lists. While the file contains only the data included in the file, the directories are basically files that contain the list of FS objects inside them. We call the entries in this list as directory entries or direntries. These direntries contain metadata about the FS object they point to name, size, location, time created, time modified, etc.","journal#Journal":"Mnemofs uses a journal. Combining the CoW tactic and a journal will allow mnemofs to be resilient to power losses. How? Well, let me explain what a journal does.So, let's say, we make a change to a file. This file now has a new location. This, as discussed above, goes on to cascade to the local root. Suppose, in the middle of the copy, there's a power loss. The entire work of updating needs to be done again. Instead what we do is that we just write the new location of the file as a log to the journal. So, when we want to get the location of the file, we first traverse the FS tree to get the old location, then traverse the journal to get the new location.This also saves from updating the parent immediately, since it doesn't need to be updated in the parent as long as the log of the update remains in the journal. Each log is appended by a checksum of it to ensure that the entire log was fully written, and that there were no power losses in between.The journal is a singly linked list made out of entire blocks, and each log and its checksum take up an entire page (for now). After the journal is full or above a certain limit, it will be flushed to empty it, but we'll discuss this later.","master-node-and-journal-flushing#Master Node and Journal Flushing":"Mnemofs has a component called a master node. This master node basically points to the root of the file system. This allows the root to be treated like any other file as far as space allocation goes.Where does the master node live? In a very special location. You see, the journal is made out of n + 2 blocks, such that the first n blocks are the only blocks that are in charge of storing the logs. The last two blocks are called the master blocks. The two master blocks are identical to each other and act as each others' backups, given that the master node allows you to make sense out of the ones and zeroes stored on your device. So, master nodes are stored in the master blocks, not too unlike how logs are stored. The entire information about the root is stored along with a checksum and a timestamp. These are serially stored, so the last valid log entry points to the actual current master node.When a journal is flushed, the entire local FS tree is updated. The update of a child means storing the updated location in the parent, which also means the parent is getting updated as well, and so on till the root has another updated copy of the root. Once we get the location of the new root, we know our flush operation has been completed. Since updating the root also means the master node is updated, a corresponding new entry is appended in both master blocks. This new entry is a new master node.We make the journal's blocks follow the same wear rules as the other blocks. The moment it is to be erased, the block is given back to the block allocator as a \"to-be erased block\", and it is only used again (after erase) after ensuring every other block in the entire device is used once, but more on this later. So, every time a journal is flushed, it has to \"move\" to a new location.Each time a journal is updated, a new master node gets written. Since a block has multiple pages, and a master node takes up one page, then there can be a pages_per_block number of master node entries, and thus a pages_per_block number of journal flushes, before the master blocks are full. Thus, the first n blocks of the journal move much more than the last two blocks. Upon the pages_per_block + 1th flush all of the n + 2 blocks are flushed, and thus the entire journal moves in this case.","least-recently-used-cache#Least Recently Used Cache":"Well, first of all, my naming sense is bad. It's the Least Recently Used Cache. You can call it a cache, as it's like a twin to it. But in the case of mnemofs, it doesn't provide any benefit of being a cache, as far as read times are concerned. So we'll just call it the LRU.Why doesn't it provide any benefit of being a cache? Well, we need to discuss what it does first. The LRU's job is to effectively store all the changes a user requests in memory. Then, say for a single file, when a lot of these changes have been stored, they are applied to the flash together. So, let's say we wait for n operations from the user before emptying the changes of a file stored in LRU. That would mark it as one operation to the flash for every n operations. We can have the entire size of the LRU to be configurable, to allow users to pick their own sweet balance between memory consumption and flash wear reduction.We still haven't answered the initial question yet. But before that, we also need to look into the structure of the LRU. The LRU is made out of something called a kernel list. For the uninterest people, it's simply a clever way to write circular doubly linked lists. The kernel list in LRU contains several nodes. Each node represents an FS object, be it a file or a directory. Now, each node itself has a kernel list as well. This list is made out of what mnemofs calls deltas. Each delta is an operation by the user. To the person who takes pain to understand the VFS methods, it is clear that the VFS methods, at their base, deal with some fundamental operations on FS objects. Namely, either reading, updating (and writing), or deleting some bytes.Mnemofs has some further restrictions on this, to allow the usage of this LRU to be a bit simpler. When it comes to updating, VFS methods can ask you to put m bytes in place of n bytes. However, the way mnemofs works, we force this to follow the condition m == n (we'll discuss about this a bit later). If you're writing to a file, it will be replacing n bytes of 0-byte filled data, with n bytes of actual data, and so on. And then there's the delete operation, where we remove n bytes of data. Both operations start at a specific offset, which menmofs calls a \"data offset\" (as it's offset inside the data stored in the CTZ lists).When an LRU receives a delta for a specific node, it will bump the node from wherever it is, to the start of the LRU. This way, the last node is always the least recently used node. When the LRU becomes full, and wants to add a new node, it will pop out the last node before adding the new node. This pop will follow with the deltas being actually written to the flash, and then having their corresponding log in the journal.Now, finally, the answer to the initial question. Well, thanks for waiting this long, but I'm afraid you'll have to wait a bit more! I'll have to explain the entire process through which all components work together to be able to make you understand, so please bear with me.","entire-process#Entire process":"Well, the user makes a syscall (directly or indirectly) to do something with a file or a directory. This is then transferred to the VFS, who find the right file system to deal with the request. How? Well, the path provided by the user, and the mount points of file systems help in this. So, let's say we're lucky the user refers to mnemofs.Mnemofs then deals with the request appropriately. Let's say, it deals with reading 5 bytes from an offset of 10 in the file. What file the user is referring to needs to be found. First, the current master node is consulted to get the location of the root. Once we get the root, we traverse the directory entries (direntries) in it to find the suitable FS object that the path refers to after the root, and so on, till we either reach the intended path, or give an error of not finding the desired FS object.What we got here might be an old location for the intended FS object, so we traverse through the journal to find the newly updated location. Once we get the new location, we read the data stored in the list. However, this data might be old as well, as the LRU contains the latest data. So, we apply the changes to the CTZ list from the LRU. We do this cleverly to allow the changes to be applied in memory, and in chunks, to set an upper bound on the memory usage.Thus, we get the updated information. Since the LRU comes last in this process, that's why it doesn't behave like a cache per se.Let's say we want to delete some data. Most of the previous process will be the same. In this case, we just add the fact that we want to delete n bytes from m offset in the node corresponding to this file as a delta, and the node will be bumped to be at the start of the LRU. If the node gets full, and another delta is to be added, the node will be emptied of all deltas, which means, they will be written to the flash. Similarly, when the LRU gets full of nodes, and another node is to be added, the last node is popped, and written to the flash. When the changes are clubbed together and written to the flash, their new location is updated in the journal as a log, followed by their checksum.Now, if the journal is above a certain limit (usually 50% full), the journal is flushed. The file system enters a state of flush. All the changes in the journal are written. However, this will cause changes in their parents, and these changes will go through the same process that all changes do in mnemofs. Then the flush will be repeated again, and again. Do note that the journal has not been erased yet. The reason why we flush the journal when it reaches a certain limit, is to keep space for the logs of their parents and ancestors during the entire flush operation. So, this way, when we finally reach to the top of the local FS tree, the new root is written, and a corresponding new master node is written.Once this master node is written, the entire flush operation is basically complete. We just wipe the entire journal clean. We do not need the logs anymore. Why? Once an FS object's latest log is written, all the logs of its children are useless, as the new location already is updated with the latest locations of its descendants. As the local root is the highest you can go in the local FS tree, once its log is written (which is the master node entry), all other logs are not required, and can be erased freely.Now, sometimes, the master nodes might be full, in which case a new journal and new master blocks are allocated space, and then the new master node is written to it. The master node also contains the location of the journal, and thus, this new journal will instantly become the new journal once the master node is written to it, and the old journal and its master blocks are given back to the block allocator to be erased.Now, till the new master node is not written, none of the pages related to old locations should be erased. Why? Imagine we have a power loss, and we want to retrieve the old state of the file system, only to find your dear block allocator has erased them thinking they won't be needing that. Turns out you do need that don't you? So, once the new master node is written, only then will any erase operations happen. During the flush operation, all the pages containing old data mark themselves as \"to-be erased\", however, only when the new master node is written, will the block allocator scan through all the pages that want to be erased to check for entire blocks that want to be erased, and then erase them.","my-pull-requests#My Pull Requests":"My contribution was to implement mnemofs to a working state. It's not feature complete yet, however, it's very close to being so. These were my contributions related to the project and tools related to my work on this project:\n#11806 drivers/mtd/mtd_nandram: Adds virtual NAND Flash simulator\n#12396 fs/mnemofs: Adds mnemofs and mnemofs journal\n#12658 fs/mnemofs: Setup and VFS methods\n#12661 fs/mnemofs: Add Block Allocator\n#12668 fs/mnemofs: Add parent iterator and path methods.\n#12680 fs/mnemofs: Add LRU and CTZ methods\n#12683 fs/mnemofs: Add journal methods.\n#12701 fs/mnemofs: Add master node and r/w methods\n#12808 fs/mnemofs: Refactoring path logic, direntry size bug fix, open free bug fix\n#12937 fs/mnemofs: Fix journal log rw issue, rw size issue\n#12943 fs/mnemofs: Autoformat\nFurther, before GSoC, I've made some contributions while getting started with the codebase:\n#11647 fs/vfat: Fix typo in the macro DIRSEC_BYTENDX\n#11656 docs/fs/vfat: Improve VFAT documentation\n#11730 fs: Add VFS docs","conclusion#Conclusion":"I've had a very great time contributing to Apache NuttX, and I plan to keep continuing my contributions to both mnemofs and the entire Apache NuttX codebase as well. I had the fortune of working under a great mentor, Alan, along with the entire (very supportive) Apache NuttX community. I can not thank them enough for giving me the chance and considering me as a suitable contributor for GSoC '24 for this ambitious project. Further, I've had the pleasure of being at the receiving end of the waterfall of knowledge that's written by the littlefs team in their design document.Last but not least, I've had the fortune of having the immense support of my parents, friends, co-contributor Rushabh, and co-workers in this entire journey, without whom I could not have done it.Thank you to you too for reading this far."}},"/mnemofs/intro_p1":{"title":"Introduction Part I | mnemofs","data":{"":"So, designing and implementing a file system...it's a daunting task, and very overwhelming as well. BUUUUUT here's me delving into the concepts to the best of my knowledge. Grab a drink or some popcorn as it's going to take a while.","storage#Non-Volatile Storage":"Non-volatile storages like hard disk drives (HDDs), Solid State Drives (SSDs), Compact Disks (CDs), Pen Drives, NAND Flash Devices, etc. are basically entire arrays that you can use to store anything.What makes it different from arrays in your Random Access Memory (RAM)? Many things. But the most important of them is that it is non-volatile. This means that if something is stored in it, it will remain stored in it even if power is no longer supplied to the device. As an additional bonus, as far as storage mediums that exist till date go, if you store even a byte at a location, say, x, then it will be at x unless deleted explicitly (or some unexpected event can modify your storage, but more about it later!).Another difference is that, usually with an operating system (OS) running on your device, in RAM, you can not really specify where you want data to be stored in terms of absolute position in the device. If you try to write, say, a byte, at position x, it needs to be within the allowed locations by the kernel of the OS. Without getting too much into it here, as it is out of scope, if you do not write within the confines specified, you get hit with bad karma in the form of Segmentation fault (core dumped).\nThere might be people trying to say you cannot do that to a non-volatile device running on an OS as a user either blah blah, while others will counter it with their own written kernel module to do it but those are technicalities, and it will only result in confusion.\nThus, having established that they are different, we'll be calling volatile memory (primary memory) like RAM as memory and non-volatile memory (or secondary memory) like HDDs, etc. as storage. Storage is slower than memory, but it is also cheaper per byte. Adding the advantage of the non-volatile nature of storage shows you why it's such a popular medium of storing things.In and of itself, storage seems like a very good thing, but it is like a fox in sheep's clothing. It can go horribly wrong if not used properly. In fact, the false sense of security of having your data persist can make the shock even bigger if you can not access your data for whatever reason due to improper use.","fs#File Systems":"","fs-need#Why are they needed?":"A thing to note is that a user would be better off printing out everything compared to managing the data on a storage device manually. Why? Here's an example to help you grasp that situation:Let's say there are 16 Bytes (16 B) of storage on a device, and you want to store 4 pieces of information, each of 4 B. If we split it into blocks of 1 B each, we can give 4 consecutive blocks to each piece and write it down somewhere, say on a piece of paper, where each piece is. Say, after some time, the 4th piece is no longer required, but your 2nd piece also suddenly needs 3 B instead of the original 4. So you will clear the 4th piece's data from the device and clear the last block given to your 2nd piece, so it only has 3 B. Now the 8th, 13th, 14th, 15th, and 16th blocks are clear of data. Now, if another new piece (5th piece) wants 2 B of storage, you give 8th and 13th place to it, and so on and on.\nIf you got confused while reading the example, that just proves my point.\nThis is very confusing to think about and, more importantly, difficult to remember. And here we are, barely talking about 16 B of information space and 4 pieces of information. Reality is harsher. We need to store SOOOOO many things. Your high-definition (HD) photos, your movies, your applications, your games, and so on. A normal user just wants to store things without pain. The above-mentioned method is very much an alternative definition of pain.\nPain always exists.\nIf you can't feel it,\nit's probably being shouldered\nby someone else.\n- Me, 2k24\nSo, some developers take that pain away from you onto their shoulders, and bring you file systems which are programs that manage your storage without you having to think about them too much.","file-system-components#File System components":"File systems have a lot of components. Like a car, some of them are visible to a user, and they are aware of their presence, while the others are kept under the hood. These components are called file system objects (FS objects).Also like the parts of a car, these fs objects have different names depending on whom you ask. We'll stick to Linux (/ Unix / POSIX) terminology here, but at the end, it is just a name.We'll go through some of the very common ones below, while we keep others for later exploration as we delve deeper into file systems.","file#File":"In a traditional sense, any collective piece of information that is part of one entity is one file. Like a photo. One entire entity. It is a very loose definition. What if you cropped an image into 4 pieces and kept them separately? You do you; no judgment.Take it like a collection of information that you want to keep together, such that programs can work on the entire thing together. An image viewer can show you an entire image in one go if you keep all of it together. A video player can play your entire movie in one go if kept in one piece. And so on.They are analogous to a single paper or multiple papers that are kept together in something called a file (usually seen in the old photos of offices from a time close to the ice age) as they belong to the same topic.","directory#Folders / Directories / Drawers":"Again, work terminology. They are called many different things. We'll stick to directories, as Linux calls them. They are just a group of files and/or other directories as well. Why are they even needed? Multiple reasons. The right answer is that it depends on the user. But, here are some of the common reasons:\nOrganization. We like to be viewed as organized intellectual beings. So we organized multiple files in a hierarchical system so that topics become more specific the further down in the hierarchy you go.\nSearching. Suppose you have 500 files in your view. If you want to look for a specific file, you need to go through, in the worst case, all of them. If organized, the search may be way faster, as you would know which directory and the same inside it, and so on.\netc.","symlinks#Symbolic Links (symlinks) / Shortcuts":"Suppose you have a file in a directory that is very deep in a hierarchical system of directories. But suppose there's this folder called \"My Desktop\" in which, if files are kept, they will be shown on...your desktop (🤯). Now, keeping everything on your Desktop would not make sense, and that file is happy where it is. And you're mostly happy with where it is as well. But you need it on your Desktop as well.Well, copy it, duh! But updating one does not update the other. Thus came symlinks. They are a type of file that points to the location of another file. There are two types of symlinks, but...technicalities.Symlinks might not have universal support as they might not be considered essential for the target audience of the OS, but most major general purpose OSes do support them.","inodes#Inodes":"Files usually have two types of data associated with them. Their content, and their metadata. Their content is what we usually refer to as files. Their metadata usually refers to information about the file. If you go back to the work terminology, it might be analogous to the information written on the cover of the physical file.These contain information like the file's name, their owner (who created it, or who owns it, etc. in a multi-user environment), their creation date, their last modification date, their last access date, their size, and so on. These are stored as another FS object called an inode.Inodes are very much optional too, and it depends on the file system on how they want to manage the metadata.","conclusion#Conclusion":"So, we glanced over the basic knowledge required to start learning about file systems. The fun and games end here, and in Part II, it's time to dive into the depths of file systems, and file system development."}},"/mnemofs/intro_p2":{"title":"Introduction Part II | mnemofs","data":{"":"We'll dive into an incremental explanation of various concepts involved with file systems and operating systems. It dives into a lot of \"suppose\" and \"what ifs\" to try and get you a sense of the problems that lurk in plain sight. Only by recognizing the existence of these problems can people begin to think about trying to solve them.\nSolutions exist\nonly after problems\nare identified.\n- Me, 2k24","fs-os#File Systems and Operating Systems":"If you develop your own OS, and if you develop your own file system (FS), and if you only want your own FS in your OS, then you can develop them pretty much however you like them. Your OS can change to suit your FS needs, or vice versa. There is no need to be considerate of users or other developers who differ in opinion. Make your own OS-FS pair if you are so much better! There is no need to be the most efficient! If it gets the job done, it is good enough!This \"true power\" is probably what was felt by the developers of DOS decades ago.The world has changed since then. For better or worse, it has become too complex and demanding for one solution to be efficient or even sufficient for all needs out there. If you're a FS developer, not only is there someone out there who might do it better, but you also need to tailor your FS to support a very specific subset of storage devices and technologies to be efficient in that subset of storage technologies.","vfs#Virtual File Systems (VFS)":"","vfs-why#Why?":"Let's say I am an OS developer, and my under-development OS has this file system that I have named abcdfs, and I have to make my OS interact with the FS using the methods that my FS exposes, like:\nint abcdfs_open_file_from_path(char *path);\nint abcdfs_close(char *path);\nint (char *path);\nIn such a world, if I am developing my own OS-FS pair, my FS doesn't really have a hard boundary on what it can or cannot do. It can try and access the internet, send an email, run your game, etc.Can you see the problem? The implementation of the communication between an OS and FS would change depending on the FS. No standardization, a free but chaotic mess.Also, early on, in the 1980s, with the rise of a lot of FSs that seemed to be more or less similar to each other, there was a rise of the opinion that an OS should allow users to choose their preferred file system.A nightmare if you combine the problems.So the OS developers put some restrictions on the FS developers in the form of standardization. Wherever there is standardization, there are restrictions, but also flexibility in choosing solutions.","vfs-what#What?":"A Virtual File System (vfs) is basically a simple interface that the OS demands an FS implement. This allows the OS to use multiple file systems at once, and execute the necessary ones.For example, if an OS defines its interface to be like this:\nstruct vfs_ops {\n    char (*open)(char *),\n    char (*close)(char *),\n};\nthen it means that it wants every file system to have at least one open and one close function. The file systems can then expose it like:\nstruct vfs_ops abcdfs_vfs_ops = {\n    .open = abcdfs_open_file_from_path,\n    .close = abcdfs_close,\n};\nThis interface is what the OS will use to interact with the file system. If a user wants to open a file, then the OS can expose a system call like int open(char *path) for the user. The internal implementation can be over-simplified to this:\nint open(char *path)\n{\n    // headache stuff\n    struct vfs_ops ops = get_vfs_ops_from_path(path); // ops == abcdfs_vfs_ops\n    ops->open(path);\n    // even more headache stuff\n}\nThis has some obvious advantages:\nThe user does not need to care about the underlying FS being used.\nThe FS developer does not need to develop their own OS.\nThe FS developer just needs to modify their existing FS to include the interface of the OS they want to run their FS on.","drivers-and-operating-systems#Drivers and Operating Systems":"Without diving into too much detail, a lot of OSes also have an interface similar to a VFS for device drivers, which includes drivers for storage devices.Now the FS developers do not have to explicitly support a very specific storage device from a very specific manufacturer, but can become more generalized. Instead of making a FS that supports only Samsung 1TiB SSDs, I can move on to making an FS that instead, say, supports only SSDs, leaving the implementation details to the drivers provided by the manufacturers.Again, redistribution of pain.So, in a way similar to the communication between OS and FS, the FS can now interact with the storage device through the OS using their respective interfaces, like:\nint abcdfs_open_from_path(char *path) {\n    // code\n    struct device dev = get_device_from_path(path);\n    if(device_type(dev) != STORAGE_DEVICE) {\n        return -EINVAL;\n    }\n    struct storage_driver_ops ops = get_storage_driver_ops_from_device(dev);\n    ops->read_at_offset_in_bytes(offset);\n    // more code\n}","entire-flow#Entire Flow":"Let's go over the entire flow.A storage device is connected to your computer or machine, and the CPU can communicate with it using the driver it provides (not going into too many details of this part here as it deals with hardware I/O).Your OS will have that one FS it supports internally. It is upto the OS developer, and can be anything...rootfs, fat32, etc. This maintains a global hierarchical system from the root /. Due to the whole \"everything in Unix is a file\" thing, your device may be visible as a device in with a path like /dev/my_device. Again, it exploits the whole VFS-is-just-an-interface thing. As long as it is follows the interface, it can still do pretty much anything on your OS, given enough permissions. A free, but not chaotic, mess 🥳.If you want to use this device with a file system, you need to mount it. So, this storage is then mounted at a certain location in your computer's location, say, /hi. This process involves mentioning the file system that you want to mount this device with, say abcdfs. This creates a directory /hi, which will now serve as the root for this storage device. The metadata about the mount process is stored by the OS (where it is stored is implementation-dependent).Now, when a user wants to, say, create a file /hi/my_file3, they will call the required system call (syscall) that the OS provides for creating a file at a given path. Inside this syscall, the OS will try to create a file /my_file3 on the storage device mounted at /hi. It will get the VFS operations of the file system that was used to mount the device at /hi and use the create method exposed by that file system to create that file.The create method exposed by the file system might itself use a write function exposed by the driver of the storage device through the mount point. A mount point contains information about the file system and the device, and thus, both the device's driver operations and the file system's operations can be used through the mount point.This is a very simplified and generalized way in which OS, FS, and devices interact with each other.","conclusion#Conclusion":"Here we learnt how FS interacts with the OS, and in Part III, we'll dive into FS and the various solutions that have emerged through the decades to an apparently \"simple problem.\""}},"/mnemofs/intro_p3":{"title":"Introduction Part III | mnemofs","data":{"":"File systems have existed for almost as long as storage mediums have, which is to say, decades. File systems started out very simple. As the needs of users increased, as operating systems evolved, and as the quirks of storage mediums increased in exchange for providing maximum efficiency under very specific conditions, file systems had to adapt, and they generally became more complex but more specific.The rest of this blog takes heavy inspiration from littlefs's design document, which is a file sytem that deserves to be put in an art gallery.","types-of-file-systems#Types of File Systems":"There have been various file systems with their various quirks, but they can be generally divided into four types.","block-based-file-systems#Block Based File Systems":"These are file systems that represent the used storage space in the form of a tree. They are also the oldest types of file systems out there.They divide the storage into various blocks in which files are stored. Any updates to the files are done in place. Let's say block x contains my file, and if I want to update the file, the same block is rewritten with the new updated file's content.Can you see the problem? Suppose my block is 256 B in size, and that contains a file of size 256 B as well. Say some random bytes, say 13th, 19th, 100th, and 105th bytes, are to be updated to new values by your computer. Let's say the writes can be done at a maximum rate of 1 B at a time. And to add to this, let's say our luck is very bad, and after it has updated the 19th block, there is a power failure, and the write operation stops, and so does your computer. Now there is no information on what changes are remaining, or what has already been written.You get a situation where some of the file is updated while the rest isn't.Nightmare.This is called non-atomicity, or non-resiliency. They are not resilient to power losses as they are not atomic. If an operation is atomic, it means that if it is interrupted in between executions, any changes it makes will be reverted back to the pre-execution state.Some examples of this type of file system are FAT and ext2.Without modifications (as seen later), these file systems do not stand up to modern needs for atomicity. You need your file to be updated, but even the previous state is better than a possibly garbled mess.While this might not seem to be much of an issue for text files, for files encoded using encoding algorithms, this is quite a hell. Depending on the encoding algorithm, it has a very likely outcome of the entire file being corrupted because the decoder can no longer decode the file and hence will give that error.Another disadvantage of such file systems can be that if there is a file that is updated very often compared to others, the location on the storage device, where the file is located, will be used much more than other areas of the device, leading to uneven wear distribution. This may end up causing that particular area of storage device to die before the others.An advantage of such file systems is speed. Due to their simple design, they are very fast.","log-based-file-systems--log-structured-file-systems#Log Based File Systems / Log Structured File Systems":"On the other extreme, there are log based file systems that take atomicity very seriously. Instead of treating the entire storage as an array, they treat it similar to a queue.Any change has a corresponding entry, which is stored in the storage device in a first-in, first-out (FIFO) manner, i.e. each new entry is stored after the end of the last stored entry. To recreate a file, all that is needed is to iterate over all of the entries in a queue. Usually each entry has a checksum of the entry suffixed to it.The checksum is usually a value obtained by hashing an entire log. While reading an entry or log, if the stored checksum does not match the calculated checksum of the log, the log is discarded, as it means that the log was not written properly. If you assume your file system is bug-free, this usually narrows the culprits to power loss.It might seem really great initially, but see the problem? It is very slow. Suppose each read (one bytes) from the device is one instruction (that's quite generous as it's 100x slower than reading a value from RAM! ), and you have a 4 GiB device. This means 4294967296 B, so that many instructions are needed to traverse the entire storage. Ignore any calculations we do with the data; this would take about 1.38 seconds if an Intel i5 10th generation was running on its base frequency of 2.9 GHz (assuming no optimisations). As mentioned, calling it one instruction per read is being generous. This means that every time you open a file, you need to wait 1.38 seconds at the very least. And moreover, the processor used for this calculation is quite a beast in itself.CPUs in embedded systems are slower. STM32F401CCU6 has a CPU with 84 MHz frequency. So, the same operation under the same assumptions would take it 48 seconds. Do remember, this is just 4 GB of storage capacity.Nightmare, but in another direction.An advantage of such file systems is of course, absolute atomicity. But another advantage is wear leveling. Since entries are added in a FIFO manner, it will ensure that any pair of blocks has a maximum wear level difference of 1. The wear levels of the blocks of storage would always look like [x+1, x+1,..., x+1, x+1, x, x, x,..., x, x], where the last x+1 wear is the location of the last entry.A disadvantage, apart from being slow, is what happens if the storage becomes full? Changes might be infinite depending on the user, and so will the entries that represent these changes. However, space is not infinite or file system development would not have been so difficult.File systems of this type include JFFS/JFFS2, YAFFS, or SPIFFS.","journalling-file-systems#Journalling File Systems":"The very weird thing is that both of the above types of file systems have mutually exclusive advantages and disadvantages. So naturally, a middle ground approach would either benefit from both, or none.We create a block-based file system, but we also reserve a certain place on the device for the FIFO queue to store logs. We call this FIFO queue the journal or bounded log. Best of both worlds. The block file system contains a sort of \"base\" state, and further changes to it are stored as entries or logs in the journal.To get the updated version of the file, all the FS has to do is take the \"base\" state and iterate over the journal, applying changes to it.Independent of implementation, there is a very strong relationship between storage location and data due to the presence of the journal. This can cause an abnormal increase in the wear of the journal along with the disadvantage of increased wear for a frequently updated file in block based file systems. Another problem is that there are essentially two file systems running in parallel, and both code complexity and execution may increase.Depending on the implementation, this may have some additional problems. The file system may decide to commit logs to the file system when the journal is full. This means that the \"base\" state needs to be updated, and thus, the journal is emptied. The changes may be committed one by one, but power loss during committing the journal may cause garbled data as well.This is the most popular category of file systems, and this category includes Linux's most popular file system ext4, and Window's most popular (actually you don't have a choice, as far as daily files go) file system NTFS.","copy-on-write-file-systems#Copy On Write File Systems":"A file system category based on an entirely new way of dealing with this problem is Copy-On-Write (CoW) file systems.What does CoW mean? You want to update a value? Copy the entire thing and write on that copy. It's similar to how some developers program before learning about the existence of version control.Suppose a block (I'll give it a name x) is at a location p. If you want to update it, you will read the entire block in memory, update the necessary parts in memory, and write the updated block to a new location q.This sounds simple, but it's a bit more complicated than that when you dive into the nuances of it.So, files and directories exist in all these types of file systems, which include CoW file systems. This means that there is a hierarchical ordering of files and directories. A directory is a collection of files, so under working CoW file system implementations, a directory has some information about the location of a file.Let's update our file! Let's assume our file is just one block in size for simplicity. So our file got updated, and its location shifted from p to q. But the directory that contains our file still points to p!!! So, we need to update our directory! But now, the parent directory of this directory faces a similar problem. This continues to propagate upward until it reaches the root of the entire file system's tree. The root has no parent, so updating it updates the tree. BUT, unless your root is confined to some specific places in the device, you would need to store where the root is located as well, and this update problem again continues until it finally reaches something that either has a single fixed location (in which case it doesn't follow CoW as it would need to be updated without changing location) or it has multiple fixed locations, in which case the file system has to figure out which one of the locations contains the most recent update.Yep...A lot of problems, and a lot of headache.An advantage of CoW file systems is that there are two copies of the block. Old and updated. If the update was not successful, the old version would be used.The wear leveling here depends on the block allocator, which is responsible for providing the location where an update should be stored. Thus, a good algorithm for the block allocator will give good and even wear (copium 🤞, but it's possible to do this, unlike other challenges that arise due to the nature of the file system).A disadvantage of CoW file systems is pretty obvious, as shown above. A simple update takes too many copies and writes due to updating all the FS objects in the file's path.Another disadvantage is that it takes up quite a lot of space to keep copies. If you don't have enough space to update a file (which includes not just the file but ancestors as well), a CoW file system is a bad choice.Another disadvantage can be that CoW file systems require a garbage collector. So, additional memory usage, and processing time. The garbage collector needs to figure out which blocks (old copies) it can safely erase, and which are required in case there is a power failure in the near future. This is not a computationally cheap thing to do.The amount of extra space used by old copies is determined by the aggressiveness of the garbage collector, but no garbage collector should erase copies of the FS objects in the path of a file until it is sure the entire file has been updated.","conclusion#Conclusion":"There are too many types of file systems, and there are too many problems. They try to solve some problem but end up creating another or solving it partially. Mnemofs is heavily inspired by littlefs, which itself tries to take the best of both CoW and journaling file systems and combine them with some ingenious problem solving.We'll look into mnemofs, and specifically, its LRU in the next part."}},"/mnemofs/journal":{"title":"Mnemofs's Journal","data":{"":"Continuing a reoccurring theme, to understand this, we need to understand something else. Since the journal stores updated information about a file or a directory, first we need to look into how mnemofs stores a file or a directory.","count-trailing-zero-ctz#Count Trailing Zero (CTZ)":"On a seemingly unrelated note, have you heard of the CTZ operation? It, as its name suggests, counts the number of trailing zeros in the binary representation of a number.e.g., 1860 is 11101000100 in binary, and there are 2 trailing zeroes in it. Thus, ctz(1860) == 2.GNU compilers provide a __builtin_ctz(x) for this, and in C 23, it's become a part of the official standard. Most CPU architectures support this instruction.","ctz-skip-list#CTZ Skip List":"A skip list is a modified singly linked list that, instead of containing one pointer per node to point to the next node, contains more pointers in addition to the original pointer. Also, as shown by littlefs (who pioneered the CTZ skip list data structure), Copy-On-Write benefits more from a backward linked list (or a backward skip list) than a forward skip list.Skip lists prefer to keep the number of pointers per node at random to lower the cost of insertion and deletion. However, a Copy-On-Write file system has no need for \"insertion\" and \"deletion.\" All of the operations are in the form of \"appending\" and all modifications requested are done in memory. We'll discuss how a CTZ skip list works in mnemofs, but first we need to know what the structure of a CTZ skip list is.In CTZ skip lists, each CTZ skip list block at index x has ctz(x) + 1 number of pointers (0 for the 0th CTZ skip list block). Each CTZ skip list block has pointer to the (x - 2^i)th CTZ skip list block for every i such that x is divisible by 2^i. For example, a CTZ skip list block with index 6 will have pointers to 5th and 4th CTZ skip list blocks, while a CTZ skip list block with index 8 will have pointers to 7th, 6th, 4th, and 0th CTZ skip list blocks.In mnemofs, each CTZ skip list block takes exactly one page of space.Since it's possible to iterate over a CTZ skip list to reach any CTZ skip list block from the very last CTZ skip list block, only the page number and index of the last CTZ skip list block are stored, along with the size of the file. Mnemofs uses CTZ skip lists like its creator, littefs, does. However, mnemofs uses it to represent files and directories.","travel-and-offset#Travel and Offset":"We'll use the word \"offset\" to refer to \"data offset,\" which is the offset into the actual data contained in the CTZ skip list, which doesn't include the pointers.Conversion of the offset into its CTZ skip list block index and page offset can be done through the derivations done by littlefs.Travel from one CTZ skip list block to the other can be done using a greedy approach that utilizes the fact that the powers of 2 that change from one CTZ skip list block to the other while traveling first monotonically increase and then monotonically decrease. The graph might be discontinuous, but that's not an issue. It's actually easier to understand by reading the code in this case.","journal-logs#Journal Logs":"Back to the journal. Now, once the updated information on the CTZ skip list is received, it is logged to the journal along with a CTZ skip list representation of the path of the FS object. This log is followed by a checksum to make sure that the entire log was written correctly to the flash.","structure#Structure":"The journal consists of blocks from the NAND flash. The last two blocks allocated for the journal are reserved for the master node and are called master blocks (more on that later). The first block starts with an 8-byte magic sequence, followed by the number of blocks (all n + 2 blocks) allocated to the journal, and then an array that contains the block numbers of all the blocks allocated to the journal.This is like a modified version of a singly linked list. A traditional singly linked list design was not used to allow the mount process to quickly find the master node once the journal was found (more on that later), and it reduces space, as a traditional design would require the last page of every block in the journal to be reserved specifically for storing the block number of the next block.The first n blocks (out of the n + 2 blocks) store the logs, and once full, the journal gets flushed, but more on that later.","conclusion#Conclusion":"So, this is how the journal works, and the \"more on that later\" parts will be explained in the next section that discusses the master node of mnemofs."}},"/mnemofs/lru":{"title":"Mnemofs Least Recently Used (LRU) Cache","data":{"":"Before diving into the LRU Cache, called LRU for short, we need to look into a data structure that every one seems to know, but with a slightly different flavor.","kernel-linked-lists#Kernel Linked Lists":"Linux, and NuttX (among others) have this very special flavor of linked lists that seem to make it just right, called a kernel linked list, or simply, a kernel list. This is a circular doubly linked list which can be used to store any data type.Below is a traditional circular doubly linked list.And the below is a kernel list.The type struct list_head contains only pointers to other struct list_head. It doesn't care about the structure it is part of. This can allow traversal, and conversion of any kind of structure we want into a list. The question is, if we have a pointer, how do we get the original structure back?The answer to that is pretty simple and brilliant. Pointer offsets. If your struct struct my_struct contains a member struct list_head list, then, let's say, the offset of list from the start of the struct is off, then if we have an address x pointing to list, we can get its parent by just doing x - off. off will always remain constant for a given struct, and there are utilities provided to calculate them. In fact, the list utilities don't require you to even have to think too much about how this works.","lru-structure#LRU structure":"Back to the LRU. The LRU is in-memory, and its main purpose is to reduce the wear of your storage device. The way it does is by bunching some changes to the same file, and then writing them all in one go. LRU bases itself off of the structure of a traditional LRU design, and so, like any good LRU, it needs a doubly linked list implementation. We'll go a bit further than that and use kernel lists.The LRU in mnemofs is a kernel list of nodes. Each node represents a file or a directory. Each node contains deltas, which are basically the updates a user wants. The deltas are arranged in a kernel list for code reduction, however, they may use something as simple as a singly linked list. Deltas are of two types: either put x bytes at an offset off by replacing at maximum x bytes (less than x bytes at the end of a file), or delete x bytes from offset off.When a new node is to be inserted, and if the LRU is full, the last node (tail of the list) is popped off, and all the deltas in it are written to the flash. This is called the flush operation. A flush operation may happen implicitly as explained, or explicitly in cases like where a file is closed.When the deltas are written to the flash in an Copy-On-Write (CoW) manner, the new location and size is changed and the journal comes into play here. This need to be updated in the parent as well. Thus the parent goes through this same procedure for updates as well.CoW file systems face a very common problem of cascading or bubbling up of updates. If a file is updated, its location changes. Thus the parent needs to be updated, and its location is changed as well, and so on this rises up the file system till the root is updated. However, unlike most CoW systems, the LRU does not let the updates bubble up into the file system immediately.The LRU isn't a cache in a strict sense, as the original data still needs to be read from the flash before applying the changes to the LRU, and thus, does not \"save\" time like usual caches. However, the main purpose it has is to batch updates together to reduce the number of times the file is updated in the flash. The size of the LRU is configurable during compile time, thus giving control over the RAM vs wear tradeoff. Also, if someone wants to apply the updates to the flash as soon as possible, then the LRU size should be kept to a minimum.Unlike traditional caches, we're quite happy to use the fact that while using caches, the original store is not up to date with the in-memory store as this stops (or rather, staggers) the upward propagation of the updates to the root. My apologies to all those great people who have worked to solve this issue in other areas of computing.\nPower loss will cause all changes in the LRU to be lost. In fact, it's the only bunch of updates that will be lost. The updates in the journal will remain, and the file system will be in a recoverable state at all times.The smaller the LRU is configured, the lesser you will lose after a power loss.","applying-updates#Applying updates":"When it's time to update convert the deltas into actual data on the flash, it's pretty complicated. The first task is to determine what doesn't need to change. If a file takes up blocks a, b, c, d and if the block c has some deltas, then the new blocks have to be like a, b, x, y due to CoW. Thus, the \"prefix\" needs to be determined, which, in this case, is a, b blocks.Now, we need to apply all updates contained in the deltas over such a file. Due to the limitation of the types of deltas, this gets much simplified into a two pointer method, where the upper limiter of the window shifts inwards by x bytes for every deletion that shifts x bytes inside the window. It might shift x + y bytes in total, where y bytes fall outside the window. The way this two pointer window algorithm works, y will not be lower than the minimum limiter of the window, but rather, can only be higher that the upper limiter. We do not care about this, as the next iteration will take care of it. Also, we increment a deleted bytes counter...say del...by x to denote how many bytes have been deleted before the start of the window. So, then, if we are considering a window x, x + m in the new file, then we need to copy the bytes x + del, x + m + del from the old file. This way del marks the amount of \"compensation\" we need to provide.After the window reaches the end of updates, it's time to save this new location in our journal."}},"/mnemofs/mideval":{"title":"Google Summer of Code '24 @ Apache NuttX, Mid-Term Evaluation Blog","data":{"":"Hi! I am Saurav Pal, a recent graduate, and a regular passionate developer interested in systems and anything low-level. I welcome you to my blog, which covers my contributions and journey as a Google Summer of Code (GSoC) Contributor for Apache NuttX in 2024, from the very start to the end. My project involves designing and implementing mnemofs, a NAND Flash File System for Apache NuttX.","my-contributions#My Contributions":"From before the start of GSoC till the midterm evaluation, I have made a few contributions to NuttX. Keeping the unrelated ones for later, here's my work on mnemofs:\n(#11806) Virtual NAND Flash device for the NuttX Simulator\n(#12658) Mnemofs setup and VFS methods\n(#12661) Mnemofs Block Allocator\n(#12668) Mnemofs parent iterator and path methods\n(#12680) Mnemofs LRU and related CTZ methods\n(#12683) Mnemofs Journal\n(#12702) Mnemofs Master Node\nThe file system is in a basic testable state, however, it's riddled with many bugs that need to be solved. The file system needs to undergo thorough testing to ensure it's reliable.","components#Components":"The files for mnemofs code have these meanings:\nmnemofs.c: Entry point to the FS. All the implementations of Virtual File System (VFS) methods are in this file.\nmnemofs_blkalloc.c: Contains Block Allocator. Provides a block or page when needed, also marks used pages, and is responsible for erasing a block when all pages in a block want to be erased.\nmnemofs_fsobj.c: This contains FS Object methods, i.e. path and parent iterator methods to the VFS methods. This abstracts away the file and directory structure in the flash and the management of on-flash data and updation of that with the LRU updates. These interact only with the LRU, which also provides wrappers for underlying methods.\nmnemofs_lru.c: Contains the LRU. This stores updates to a file or directory and bunches up the updates to apply batches of updates in one go, to reduce the number of writes to the flash, or allocations of blocks or pages. This helps in reducing the wear. A higher chance of power loss should be accompanied by a lower configured LRU size. The LRU stays in the memory and interacts with the flash through the CTZ methods. This provides a wrapper over CTZ methods as well, to make it simpler for FS objects to get data with LRU updates applied.\nmnemofs_ctz.c: Contains CTZ methods. These are used to directly write/read CTZ information to/from the flash. This lies as a wrapper over the journal and will update, or be updated from the journal. These abstract away the data portion of the CTZ skip list, and don't let the caller worry about the pointers, and instead treat the data portion across blocks (of varying sizes) as a single unit that can be used with an offset from the start. The journal should not be accessed directly, but instead through this.\nmnemofs_journal.c: Contains the Journal. This contains a lot of logs in a sequential manner. Each log contains a path of the old file in the form of an array of CTZ list information of all the FS objects in the path and the new CTZ list information, and this is appended by a checksum of the log.\nmnemofs_rw.c: Contains the raw read, write, erase, check bad block, and mark bad block methods. These will only be used by the CTZ methods and the journal. These are very simple and are almost just a wrapper over the underlying device driver's exposed methods.\nA detailed blog on the various components can be found here.","key-decisions#Key Decisions":"Some decisions make the implementation differ from the initial design of mnemofs:\nJournal: The journal was initially meant to serve as a circular singly linked list. This would be problematic for a lot of reasons. This would mean that every block would contain the page number, most logically at the very end or the start. Journal blocks are allocated at the start, during the binding phase of a file system. As you can only write to a page once (before erasing), it means that the block numbers have to be written somewhere right at the start. The block numbers are 4 bytes in length, compared to a usual minimum of 512 byte page length. Thus, an entire page per block would be waster for this. So, it was decided that there would be an entire array containing the block numbers in the journal, and it would be preceded by a count of the number of blocks. Since it's a block that has a large size, it's assumed that the array will not extend past a single block. If the array ends in the middle of a page, the rest of the page is left unutilized. Thus, we waste a maximum of slightly less than 1 page worth of storage, compared to the number of blocks worth of pages.\nLRU: The LRU earlier contained a generic interfacing of exchanging x bytes with y bytes. However, applying these commits to an array becomes a nightmare with limited memory usage. Thus, it was narrowed down to two cases... x bytes replaced with x bytes in the updated file (unless it's the end of the file), or x bytes replaced with 0 bytes in the updated file. This made the entire process much simpler, reducing it to be solved with a simple 2-pointer approach.\nWrapper Approach: Almost everything is a wrapper to another layer of methods, even in situations where it simply just calls the corresponding underlying method itself. This was done to reduce cases where one function would, say, update location from the Journal, but one would not, and you need to keep that in your mind, or make mistakes. Here, uniformity helps clear doubts.","my-journey#My Journey":"Disclaimer: Contains a lot of rants and yapping.","pre-gsoc#Pre-GSoC":"","pre-apache-nuttx-period#Pre-Apache NuttX Period":"Before the GSoC period, I was looking into Linux, trying to get started as a contributor. But first, I needed to have a fair understanding of how the whole thing works. Baby steps, you know. Read the famous Robert Love's book, saw a lot of videos, and of course, tried to read the codebase.It was overwhelming, and it was stressful, because... I mean, just look at Linux's codebase and its history. I was also interested in embedded systems, but I was trying to do one thing at a time. While I was going through the codebase, I was hooked on to the history of file systems in the codebase. You could see new file systems appear and some old ones disappear every few versions. Over that, I had heard file systems are the lowest in the whole tree, and are kind of only dependent on the Virtual File System and whatever the storage driver is, and that too that file system developers do not need to worry too much about the storage device (as long as it's of the same technology, but I didn't know this then), or the internals of the OS.So, I felt file systems would be a great starting point. Started reading the codebases of the various file systems. After 3-4 file systems, all of them seemed to swim in front of my eyes and feel the same, and concepts from one FS seemed to merge with concepts from another FS 🫠. Still, carried on, as I knew if I tried to read it repeatedly, it would start making sense someday. Every line of C makes sense, as mostly nothing is hidden away, and more so in the case of OS codebases. If it were a higher level language, I would have just probably given up, idk.I wanted to apply for GSoC once again for 2024, and this time, for a large project (I had done a medium project for PostgreSQL in 2023). Around this time, I thought of looking into any OS-related projects that are available, as those align with my interests. Found Apache NuttX. What does NuttX and the project I found have in common with my interests? Everything! File System + Operating System + Embedded Systems. All my boxes ticked, and a match made in heaven, at least from my side 😉.","first-pr#First PR":"So, I decided to look into Apache NuttX and try to understand it. I tried looking into good first issues, but honestly, I was just too optimistic at that point and even the scant few first issues were very advanced for the past me. I could successfully set up NuttX, and build the simulator configuration (I did not have any embedded boards available, but more on that later).I did not wish for hand-holding, as it's often detrimental to the learning process, but I did wish for a nudge in the right direction. And so I contacted Alan who was listed as a mentor to the project on any kind of resources I could gather. He very generously provided some resources and suggested I should look into existing FS codebases (like I had done in Linux).The difference between Linux and NuttX codebases? NuttX codebase looks beautiful. Sure, Linux might look beautiful to the experts, but NuttX looks good even to newbies like me. A lot of the code is properly documented...each function, what it does, and even sometimes, why it exists.I looked at the FAT file system code. In and of itself, the code isn't too complicated. But NuttX's contribution style made a lot of things clear to someone like me who was mostly stumbling around in the dark, trying to make sense of my surroundings. Finally, while reading the codebase, I came across a bug that initially I thought was something that I couldn't understand the meaning of. After asking around on the mailing list, I found out that this bug went unnoticed because the particular way the macro was used in the codebase till then made the expansion correct, even if the macro itself was incorrect, and might cause problems if the usage of the macro was modified. So, I patched it, and made my first Apache NuttX PR!","more-contributions#More contributions":"Alan had mentioned that the FAT file system documentation was a bit lacking and so since I was already looking at the file system, writing it down somewhere would be a good idea, and why not the official documentation :D. So, I researched FAT FS (or, rather VFAT), and alongside reading the code, I wrote down whatever I learned, and finally updated the documentation in my second PR.Now I wanted to move higher in the OS tree. So, I thought about learning about the VFS. Since this VFS shares a lot with Linux's structure, I could apply the VFS resources for Linux here. Along with this, I looked into the codebase and learned more about the VFS. Of course, not all of it, but a lot of it. I updated the documentation for the VFS as my third PR.","gsoc-community-bonding-period#GSoC Community Bonding Period":"This was the community bonding period. Asking questions, talking with the mentor, getting comfortable, exploring the solution proposed, and its feasibility, and exploring any improvements, and not to forget, setting up everything.","nand-flashes#NAND Flashes":"I had an STM32F401 board, but the trouble with it stemmed from the fact that its headers were not soldered. It was quite a trouble getting them soldered, but I had it ready around this time. I had ordered this before my interactions with the NuttX community and Alan, and it was just a buy-whatever-feels-popular-and-cheap. Surprise! It doesn't have a JTAG programmer, and I need to buy one. Another infinite waiting period for that to get delivered to my remote campus.I was sick of this wait and wanted to learn about NAND flashes, and thus, about drivers in NuttX. So my thought process went like this: \"I am already using the simulator. Why not simulate NAND Flash in it? I have already seen QEMU simulate chips. If CPUs, why not peripherals.\" And so, after some headache-filled days, I made my fourth set of PRs (this and this) to add a NAND Flash virtual device as an app in the simulator.This made me learn about both NAND Flashes, as well as a lot of insight into drivers' work in NuttX. Both are very critical for my understanding if I am to develop a file system for NuttX.","setting-up-nuttx-on-my-board#Setting up NuttX on my board":"So, I came back from my campus, to my home, at this time. I have the JTAG programmer and an STM32F401 board. I tried flashing NuttX in it. The LED kept blinking, and Alan said it was because it was crashing (and restarting continuously). It was very weird, because the same NuttX configuration worked for his board, but not mine, which were both STM32F401 boards. To this day, it's a mystery, but it's probably because it's a locally produced cheap copy of the board in my case. Over that, I didn't have a USB-Serial Adapter, so needed to buy one as well.So I got my hands on another board, an STM32F103. This time it was a no-go either. After much looking around, Alan pointed out a frequent issue of the wrong R10 pull-up resistor. While a solution can be found here, I could not modify the board as I was just borrowing it.So, I bought an ESP32-DevKit-V1, and while it's probably a local copy as well, it works with NuttX.","nand-flash#NAND Flash":"Acquiring the NAND flash was a difficult task. None were locally available. Absolutely none. The scant few online were of WSON8 type, which would be hard for me to solder, but they also were of a different voltage specification. I had to import them and bought a W25N from Mouser. Import duty in India is hell, and it cost me 3 times the actual cost of the components.Now, soldering this one was problematic as well. My jumper wires would not have solder stuck to them, nor did I have a breakout board that could fit it. So, I had to improvise. Since the desoldering wick contains copper, which is conductive, I broke a small part of it into 8 small wires (only 8 are required for connection) and then soldered them onto the NAND chip, from these wires, I attached the jumper wires by soldering them to these copper bits.Behold Spider NAND :D.","till-mid-term-evaluation#Till Mid-Term Evaluation":"Till the mid-term evaluation, my PR spree had come to a hiatus. This is because it's very tough to test individual parts of the file system. Almost everything is interdependent, and thus testing one without completing the other gains no knowledge about how it will behave together. Thus, the file system had to be written first. The entirety of it. At least, to a working state.Thus, that's what I did. Wrote it down all.Then, split it into multiple PRs.","apache-workshop#Apache Workshop":"I participated in Apache NuttX International Workshop 2024 as a speaker, talking about my project, and it was amazing. So many opinions, and amazing projects related to NuttX, it was amazing to see them.","mid-term-evaluation-result#Mid-Term Evaluation Result":"Passed :D.","acknowledgement#Acknowledgement":"First up, a thank you to my parents. Words can never be enough for this, and I can never thank them enough for this.Next, a big thank you to Alan, who's my mentor. Spending so much time patiently mentoring and encouraging me, even after multiple blunders along the path...couldn't have asked for more. I have gained a new perspective on a lot of open-source-related things due to the various talks and exposure.Also, a big thank you to Lup and Xiang who have helped me whenever I approached them with my doubts as well.Finally, a thank you to my friends whose support helped me with everything, and to my co-contributor Rushabh Gala for the support as well."}},"/mnemofs/mn":{"title":"Mnemofs's Master Node","data":{"":"The master node in mnemofs is the entity that can be used to get the location of the root of the file system.When the journal is flushed (or, rather, the first n blocks are flushed), then all of the changes in the journal are applied to the entire file system in a true CoW way, and this finally cascades to the root of the file system, till the root is updated.At this state, both the old data and the new data exist simultaneously, and the new location of the root is recorded in a new master node and written to the flash. Once the new master node is written to the flash, only the entire file system's \"base state\" gets updated, and now there is no risk of losing changes or leaving the device in an unrecoverable state (at least, till new updates come in). The block allocator ensures that both the data exist together and that old pages are not erased until the new master node is written which can allow for rollback of changes.The master blocks are duplicates of each other, and contain multiple instances of the master node. The master node takes no more than a single page and contains information about the root, in a way similar to a parent directory. The master blocks contain various \"revisions\" (or instances) of the master blocks. It's important to note that only the latest master block is a \"valid\" master block. This means that all older master blocks do not point to a valid file system.","journal-flush#Journal Flush":"The master blocks contain multiple master nodes. When the first n blocks of the journal are flushed, a new master node entry is created. Thus, a master block can hold pages_per_block number of master nodes. This also means that the master node does not need to be erased for every journal flush, but rather, in integral multiples of flushes. Thus, there are some times when the journal moves alone, while at others, the journal moves along with the msater blocks, and so, master blocks move much less that the other journal blocks.","conclusion#Conclusion":"Here you saw how the journal of mnemofs works. Now, moving on to the block allocator."}},"/mnemofs/mnemofs":{"title":"mnemofs","data":{"":"Mnemofs is a NAND Flash File System designed and built for NuttX as part of the Google Summer of Code 2024 program. Mnemofs is designed to take a middle ground between flash storage consumption, memory consumption, wear and speed. It sacrifices a little bit of everything, and ends up being acceptably good in all of them, instead of sacrificing multiple aspects, and being good in one.Mnemofs consists of several components, and thus, we approach it one by one. Baby steps.Here are the blogs that cover this in depth:\nIntroduction Part I\nIntroduction Part II\nIntroduction Part II\nMnemofs\nMnemofs LRU\nMnemofs Journal\nMnemofs Master Node\nMnemofs Block Allocator\nGSoC '24 @ Apache NuttX, Mid Term Evaluation Blog\nGoogle Summer of Code '24 @ Apache NuttX Final Report\nThe journey to design and implement this wasn't smooth sailing, and required modifications pretty frequently to either make the FS faster in some respect, or to patch up gaping holes in its logic. All of this will be covered in the mid term and end term blogs for my time as a GSoC Contributor for Apache NuttX."}}}