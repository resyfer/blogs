{"/coming_soon":{"title":"Coming Soon","data":{}},"/":{"title":"resyfer's Dev Blog","data":{"":"Hi, I am Saurav Pal, and I code under the name resyfer. Here's a blog to share my two cents on stuff.","about-me#About Me":"I'm an enthusiastic developer who's open to learning anything and everything related to operating systems, embedded systems, and networking, and I like open source.I am a Google Summer of Code 2024 (GSoC '24') contributor to Apache organization in which I am working on designing and implementing mnemofs, which is a filesystem for NAND Flashes for the Apache NuttX Real Time Operating System (RTOS).I'm also currently an upcoming Associate Software Engineer at Oracle.I was also a GSoC '23 contributor to PostgreSQL as well, working on improving PostgreSQL version support for pgexporter.I am currently a fresh graduate from the National Institute of Technology, Silchar, which is one of the Institutions of National Importance in India, in which I was pursing a Bachelor of Technology (B. Tech.) in Computer Science and Engineering (CSE) for 4 years (from Nov 2020 - May 2024).","blogs#Blogs":"Here are my blogs.\nmnemofs"}},"/mnemofs/ba":{"title":"Mnemofs's Block Allocator","data":{"":"The block allocator of mnemofs is inspired from littlefs a lot, but it differs a lot. Since NAND flashes are a common device used for sensors, allocation of blocks or pages need to be fast. This means shifting the RAM vs speed tradeoff a little bit, such that it consumes more RAM.The block allocator wants to be fair and spread wear as evenly as possible. So, like littlefs's block allocator, this starts from a random block, and keeps assigning pages. Now, in mnemofs, pages and blocks can both be expected from the block allocator. In this case, the block allocator skips pages till the start of the next nearest block, and allocates from there. Since most block allocations are done by internal data structures, and they happen in bulk, skipped pages are not too much in count. Contiguous allocation may happen, but it may not as well.The block allocator contains a bitmap of each page to denote if it's being used or not. The block allocator also contains an array of counters for each block in the device. These counters count the number of pages inside that block that want to be deleted. If all of the pages inside a bllock want to be erased, then the next time the block allocator comes across this block (or any page in it), it will immediately erase the block and then allocate blocks or pages.","conclusion#Conclusion":"So, all in all, this are the ins and outs of mnemofs. A weird place to end the series? There are follow up blogs of my journey designing and implementing this!"}},"/mnemofs/intro_p1":{"title":"Introduction Part I | mnemofs","data":{"":"So, designing and implementing a file system...a daunting task, and very overwhelming. BUUUT here's me delving into the concepts to the best of my knowledge. Grab a drink or a popcorn as it's gonna take a while.","storage#Non-Volatile Storage":"Non-volatile storages like hard disk drives (HDDs), Solid State Drives (SSDs), Compact Disks (CDs), Pen Drives, NAND Flash Devices, etc. are basically entire arrays that you can use to store anything.What makes it different from arrays in your Random Access Memory (RAM)? Many things. But most important of them is that it is non-volatile. This means that if something is stored in it, it will stay on it even if power is no longer supplied to the device. As an additional bonus, as far as storage mediums that exist till date go, if you store even a byte at a location, say, x, then it will be at x unless deleted explicitly (or some unexpected event can modify your storage, but more about it later!).Another difference is that, usually with an operating system (OS) running on your device, in RAM, you can not really specify where you want a data to be stored in terms of absolute postition in the device. If you try to write, say a byte, at position x, it needs to be within the allowed locations by the kernel of the OS. Without getting too much into it here as it is out of scope, if you do not write within the confines specified, you get hit with bad karma in the form of Segmentation fault (core dumped).\nThere might be people trying to say you can not do that to a non-volatile device running on an OS as a user either blah blah, while others will counter it with their own written kernel module to do it but those are technicalities, and it will only result in confusion.\nThus, having established that they are different, we'll be calling volatile memory (primary memory) like RAM as memory, and non-volatile memory (or secondary memory) like HDDs, etc. as storage. Storage is slower than memory, but it is also cheaper per byte. Adding the advantage of non-volatile nature of storage shows you why it's such a popular medium of storing things.In of itself, storage seems like a very good thing...but it is a fox in sheep's clothing. It can go horribly wrong if not used properly. In fact, the false sense of security of having your data persist can make the shock even bigger if you can not access your data for whatever reason due to improper use.","fs#File Systems":"","fs-need#Why are they needed?":"A thing to note is that a user would be better off printing out everything compared to managing the data on a storage device manually. Why? Here's an example to help you grasp that situation.Let's say there are 16 Bytes (16B) of storage on a device, and you want to store 4 pieces of information, each of 4B. If we split it into blocks of 1B each, we can give 4 consecutive blocks to each piece, and write it down somewhere, say on a paper, on which piece is where. Say, after sometime, 4th piece is no longer required, but your 2nd piece also suddenly needs 3B instead of the original 4. So you will clear the 4th piece's data from the device, and clear the last block given to your 2nd piece, so it only has 3B. Now the 8th, 13th, 14th, 15th, and 16th blocks are clear of data. Now, if another new piece (5th piece) wants 2B of storage, you give 8th and 13th place to it, and so on and on.\nIf you got confused while reading the example, that just proves my point.\nThis is very confusing to think and more importantly, difficult to remember. And here we are, barely talking about 16 B of information space and 4 pieces of information. Reality is harsher. We need to store SOOOOO many things. Your high definition (HD) photos, your movies, your applications, your games, and so on. A normal user just wants to store things without pain. The above mentioned method is very much an alternative definition of pain.\nPain always exists.\nIf you can't feel it,\nit's probably being shouldered\nby someone else.\n- Me, 2k24\nSo, some developers take that pain away from you onto their shoulders, and bring you file systems which are programs that manage your storage without you having to think about them too much.","file-system-components#File System components":"File systems have a lot of components. Like a car, some of them are very much visible to a user and they are aware of their presence, while the others are kept under-the-hood. These components are called file system objects (fs objects).Also like the parts of a car, these fs objects have different names depending on whom you ask. We'll stick to Linux (/ Unix / POSIX) terminology here, but at the end it is just a name.We'll go through some of the very common ones below, while we keep others for later exploration as we delve deeper into flie systems.","file#File":"In a traditional sense, any collective piece of information that is part of one entity is one file. Like, a photo. One entire entity. It is a very loose definition. What if you cropped an image into 4 pieces and kept them separately? You do you, no judgement.Take it like a collection of information that you want to be kept together, such that programs can work on the entire thing together. An image viewer can show you an entire image in one go if you keep all of it together. A video player can play your entire movie in one go if kept in one piece. And so on.They are analogous to a single paper, or multiple papers that are kept together in thing called a file (usually seen in the old photos of offices from a time close to the ice age) as they belong to the same topic.","directory#Folders / Directories / Drawers":"Again, work terminology. They are called many different things. We'll stick to directories, as Linux calls it. They are just a group of files and/or other directories as well. Why are they even needed? Multiple reasons. Right answer is that it depends on the user. But, here are some of the common reasons:\nOrganization. We like to be viewed as organized intellectual beings. So we organized multiple files in a hierarchical system so that topics become more specific the further down in the hierarchy you go.\nSearching. Suppose you have 500 files in your view. If you want to look for a specific file, you need to go through, in the worst case, all of them. If organized, the search may be way faster as you would know which directory, and the same inside it, and so on.\netc.","symlinks#Symbolic Links (symlinks) / Shortcuts":"Suppose you have a file in a directory that is very deep in a hierarchical system of directories. But suppose there's this folder called \"My Desktop\" in which, if files are kept, will be shown on...your Desktop (ðŸ¤¯). Now, keeping everything on your Desktop would not make sense, and that file is happy where it is. And you're mostly happy with where it is as well. But you need it on your Desktop as well.Well, copy it, duh! But updating one does not update the other. Thus, came symlinks. They are a type of file which points to the location of another file. There are two types of symlinks, but technicalities.Symlinks might not have universal support as they might not be considered essential for the target audience of the OS, but most major general purpose OSes do support them.","inodes#Inodes":"Files usually have two types of data associated with them. Their content, and their metadata. Their content is what we usually refer to as files. Their metadata usually refers to the information about the file. If you go back to the work terminology, it might be analogous to the information written on the cover of the physical file.These contain information like the file's name, their owner (who created it, or who owns it, etc. in a multi-user environment), their creation date, their last modification date, their last access date, their size, and so on. These are stored as another fs object called an inode.Inodes are very much optional too, and it depends on the file system on how they want to manage the metadata.","conclusion#Conclusion":"So, we glanced over the basic knowledge required to start learning about file systems. The fun and games end here, and in Part II, it's time to dive into the depths of file systems, and file system development."}},"/mnemofs/intro_p2":{"title":"Introduction Part II | mnemofs","data":{"":"We'll dive into an incremental explanation of various concepts involved with file systems and operating systems. It dives into a lot of \"suppose\" and \"what ifs\" to try and get you the sense of the problems that lurk in plain sight. Only by recognizing the existence of these problems can people begin to think about trying to solve them.\nSolutions exist\nonly after problems\nare identified.\n- Me, 2k24","fs-os#File Systems and Operating Systems":"If you develop your own OS, and if you develop your own file system (FS), and if you only want your own FS in your OS, then you can develop them pretty much however you like it. Your OS can change to suit your FS needs, or vice versa. No need to be considerate of users or other developers who differ in opinions. Make your own OS-FS pair if you are so much better! No need to be the most efficient! If it gets the job done, it is good enough!This \"true power\" is what probably was felt by the developers of DOS decades ago.The world has changed since then. For the better or worse, it has become too complex and too demanding for one solution to be efficient or even sufficient for all needs out there. If you're a FS developer, not only is there someone out there who can do it better, you need to tailor your FS to support a very specific subset of storage devices / technologies to be efficient in that subset of storage technologies.","vfs#Virtual File Systems (VFS)":"","vfs-why#Why?":"Let's say I am an OS developer, and my under-development OS has this file system that I have named abcdfs, and I have to make my OS interact with the FS using the methods that my FS exposes like:\nint abcdfs_open_file_from_path(char *path);\nint abcdfs_close(char *path);\nint (char *path);\nIn such a world, if I am developing my own OS-FS pair, my FS doesn't really have a hard boundary on what it can or can not do. It can try and access the internet, or send an email, run your game, etc.Can you see the problem? The implementation of the communication between an OS and FS would change depending on the FS. No standardization, a free but chaotic mess.Also, early on, in the 1980s, with the rise of a lot of FSs that seemed to be more or less similar to each other, there was a rise of an opinion that an OS should allow users to choose their preferred file system.A nightmare if you combine the problems.So the OS developers put some restrictions on the FS developers in the form of standardization. Wherever there is standardization, there are restrictions, but also flexibility in choosing solutions.","vfs-what#What?":"A Virtual File System (vfs) is basically a simple interface that the OS demands an FS to implement. This allows the OS to use multiple file systems at once, and execute the necessary one.For example, if an OS defines its interface to be like this:\nstruct vfs_ops {\n    char (*open)(char *),\n    char (*close)(char *),\n};\nthen it means that it wants every file system to have one open, and one close functions at least. The file systems can then expose it like:\nstruct vfs_ops abcdfs_vfs_ops = {\n    .open = abcdfs_open_file_from_path,\n    .close = abcdfs_close,\n};\nThis interface is what the OS will use to interact with the file system. If a user wants to open a file, then the OS can expose a system call like int open(char *path) for the user. The internal implementation can be over-simplified to this:\nint open(char *path)\n{\n    // headache stuff\n    struct vfs_ops ops = get_vfs_ops_from_path(path); // ops == abcdfs_vfs_ops\n    ops->open(path);\n    // even more headache stuff\n}\nThis has some obvious advantage:\nThe user does not need to care about the underlying FS being used.\nThe FS developer does not need to develop their own OS.\nThe FS developer just needs to modify their exsting FS to include the interface of the OS they want to run their FS on.","drivers-and-operating-systems#Drivers and Operating Systems":"Without diving into too much detail, a lot of OSes also have an interface similar to a VFS for device drivers, which includes drivers for storage devices.Now the FS developers do not have to explicitly support a very specific storage device from a very specific manufacturers, but can become more generalized. Instead of making a FS that supports only Samsung 1TiB SSDs, I can move on to making an FS that instead, say, supports only SSDs, leaving the implementation details to the drivers provided by the manufactureres.Again, redistribution of pain.So in a way similar to the communication between OS and FS, the FS can now interact with the storage device through the OS using their respective interfaces like:\nint abcdfs_open_from_path(char *path) {\n    // code\n    struct device dev = get_device_from_path(path);\n    if(device_type(dev) != STORAGE_DEVICE) {\n        return -EINVAL;\n    }\n    struct storage_driver_ops ops = get_storage_driver_ops_from_device(dev);\n    ops->read_at_offset_in_bytes(offset);\n    // more code\n}","entire-flow#Entire Flow":"Let's go over the entire flow.A storage device is connected to your computer or machine, and the CPU can communicate with it using the driver it provides (not going into too many details of this part here as it deals with hardware I/O).Your OS will have that one FS it supports internally. It is upto the OS developer, and can be anything...rootfs, fat32, etc. This maintains a global hierarchical system from the root /. Due to the whole \"everything in Unix is a file\" thing, your device may be visible as a device in with a path like /dev/my_device. Again, it exploits the whole VFS-is-just-an-interface thing. As long as it is follows the interface, it can still do pretty much anything on your OS given enough permissions. A free, but not chaotic mess ðŸ¥³.If you want to use this device with a file system, you need to mount it. So, this storage is then mounted at a certain location in your computer's location, say, /hi. This process involves mentioning the file system that you want to mount this device with, say abcdfs. This creates a directory /hi which will now serve as the root for this storage device. The metadata about the mount process is stored by the OS (where it is stored is implementation dependent).Now, when a user wants to, say, create a file /hi/my_file3, they will call the required system call (syscall) that the OS provides for creating a file at a given path. Inside this syscall, the OS will try to create a file /my_file3 to the storage device mounted at /hi. It will get the VFS operations of the file system that was used to mount the device at /hi, and use the create method exposed by that file system to create that file.The create method exposed by the file system might itself use a write function exposed by the driver of the storage device through the mount point. A mount point contains information of the file system and the device, and thus, both the device's driver operations, and file system's operations can be used through the mount point.This is a very simplified and generalized way in which OS, FS and devices interact with each other.","conclusion#Conclusion":"Here we learnt how FS interacts with the OS, and in Part III, we'll dive into FS and the various solutions that have emerged through the decades to an apparently \"simple problem\"."}},"/mnemofs/intro_p3":{"title":"Introduction Part III | mnemofs","data":{"":"File systems have existed for almost as long as storage mediums have, which is to say decades. File systems started out very simple. As the needs of users increased, and as operating systems evolved, and as the quirks of storage mediums increased in exchange for providing maximum efficiency under very specific conditions, file systems had to adapt, and they generally became more complex, but more specific.The rest of this blog takes heavy inspiration from littlefs's design document, which is a file sytem that deserves to be put in an art gallery.","types-of-file-systems#Types of File Systems":"There have been various file systems with their various quirks, but they can be generally divided into four types.","block-based-file-systems#Block Based File Systems":"These are file systems that represent the used storage space in the form of a tree. They are also the oldest types of file systems out there.They divide the storage into various blocks in which files are stored. Any updates to the files are done in-place. Let's say the block x contains my file, and if I want to update the file, the same block is re-written with the new updated file's content.Can you see the problem? Suppose my block is 256 B in size, and that contains a file of size 256 B as well. Say some random bytes, say 13th, 19th, 100th and 105th bytes, are to be updated to new values by your computer. Let's say the writes can be done at a maximum rate of 1 B at a time. And to add to this, let's say our luck is very bad, and after it has updated the 19th block, there is a power failure, and the write operation stops, and so does your computer. Now there is no information on what changes are remaining, or what have been already written.You get a situation where some of the file is updated, while the rest of it isn't.Nightmare.This is called non-atomicity, or non-resiliency. They are not resilient to power losses as they are not atomic. If an operation is atomic, it means that if it is interrupted in between execution, any changes it makes will be reverted back to the pre-execution state.Some examples of this type of file systems are FAT, ext2.Without modifications (as seen later), these file systems do not stand up to modern needs of atomicity. You need your file to be updated, but even the previous state is better than a possibly garbled mess.While this might not seem to be much of an issue for text files, for files encoded using encoding algorithms, this is quite a hell. Depending on the encoding algorithm, it has a very likely outcome of the entire file being corrupted because the decoder can no longer decode the file and hence will give that error.Another disadvantage of such file systems can be that if there is a file that is updated very often compared to others, the location on the storage device, where the file is located, will be used much more than other areas of the device, leading to uneven wear distribution. This may end up causing that particular area of storage device to die before the others.An advantage of such file systems is speed. Due to simple design, they are very fast.","log-based-file-systems--log-structured-file-systems#Log Based File Systems / Log Structured File Systems":"On the other extreme, there are log based file systems that take atomicity very seriously. Instead of treating the entire storage as an array, they treat it similar to a queue.Any change has a corresponding entry, which is stored in the storage device in a first in first out (FIFO) manner, ie. each new entry is stored after the end of the last stored entry. To recreate a file, all that is needed is to iterate over all of the entries in a queue. Usually each entry has a checksum of the entry suffixed to it.The checksum is usually a value obtained by hashing an entire log. While reading an entry/log, if the stored checksum does not match with the calculated checksum of the log, the log is discarded as it means that the log was not written properly. If you assume your file system is bug-free, this usually narrows the culprits to power loss.It might seem really great initially, but see the problem? It is very slow. Suppose each read (one bytes) from the device is one instruction (that's being quite generous as it's 100x slower than reading a value from RAM!), and you have a 4 GiB device. This means 4294967296 B, so that many instructions needed to traverse the entire storage. Ignore any calculations we do with the data, this would take about 1.38 seconds if an Intel i5 10th gen was running on its base frequency of 2.9 GHz (assuming no optimisations). As mentioned, calling it one instruction per read is being generous. This means that every time you open a file, you need to wait 1.38 seconds at the very least. And moreover, the processor used for this calculation is quite a beast in itself.CPUs in embedded systems are slower. STM32F401CCU6 has a CPU with 84MHz frequency. So, the same operation under the same assumptions would take it 48 seconds. Do remember, this is just 4 GBs of storage capacity.Nightmare, but in another direction.An advantage of such file systems is of course, absolute atomicity. But another advantage is wear levelling. Since entries are added in a FIFO manner, it will ensure that any pair of blocks has a maximum wear level difference of 1. The wear levels of the blocks of the storage would always look like [x+1, x+1, ..., x+1, x+1, x, x, x, ..., x, x], where the last x+1 wear is the location of the last entry.A disadvantage apart from being slow is...what happens if the storage becomes full? Changes might be infinite depending on the user, and so will the entries that represent these changes. However, space is not infinite, or file system development would not have been so difficult.File systems of this type include JFFS/JFFS2, YAFFS, or SPIFFS.","journalling-file-systems#Journalling File Systems":"The very weird thing is that both the above types of file systems have mutually exclusive advantages and disadvantages. So naturally, a middle ground approach would either benefit from both, or none.We create a block based file system, but we also reserve a certain place in the device for the FIFO queue to store logs. We call this FIFO queue as the journal or bounded log. Best of both worlds. The block file system contains a sort of \"base\" state, and further changes to it are stored as entries or logs in the journal.To get the updated version of the file, all the FS has to do is to take the \"base\" state, and iterate over the journal applying changes to it.Independent of implementation, there is a very strong relationship between storage location and data due to the presence of the journal. This can cause an abnormal increase in wear of the journal along with the disadvantage of increased wear for a frequently updated file for block based file systems. Another problem is that there are essentially two file systems running in parallel, and both the code complexity and execution may increase.Depending on the implementation, this may have some additional problems. The file system may decide to commit logs to file system when the journal is full. This means that the \"base\" state needs to be updated, and thus, the journal emptied. The changes may be committed one by one, but power loss during committing the journal may cause garbled data as well.This is the most popular category of file systems, and this category includes Linux's most popular file system ext4, and Window's most popular (actually you don't have a choice, as far as daily files go) file system NTFS.","copy-on-write-file-systems#Copy On Write File Systems":"A file system category based on an entirely new way of dealing with this problem is Copy-On-Write (CoW) file systems.What does CoW mean? You want to update a value? Copy the entire thing and write to that copy. It's similar to how some developers program before learning about the existence of version control.Suppose a block (I'll give it a name x) is at a location p. If you want to update it, you will read the entire block in memory, update the necessary parts in memory, and write the updated block to a new location q.This sounds simple, but it's a bit more complicated than that when you dive into the nuances of this.So, files and directories exist in all these types of file systems, which includes CoW file systems. This means that there is a hierarchical ordering of files and directories. A directory is a collection of files, so under working CoW file system implemenations, a directory has some information about the location of a file.Let's update our file! Let's assume our file is just one block in size for simplicity. So our file got updated, and its location shifted from p to q. But the directory that contains our file still points to p!!! So, we need to update our directory! But now, the parent directory of this directory faces a similar problem. This continues to propagate upward till it reaches the root of the entire file system's tree. The root has no parent, so updating it updates the tree. BUT, unless your root is confined to some specific places in the device, you would need to store where the root is located as well, and this update problem again continues until it finally reaches something that either has a single fixed location (in which case it doesn't follow CoW as it would need to be updated without changing location), or it has multiple fixed locations, in which case, the file system has to figure out which one of the locations contains the most recent update.A lot of problems, a lot of headache.An advantage about CoW file systems is that there are two copies of the block. Old and Updated. If the update was not successful, the old would be used.The wear levelling here depends on the block allocator, which is responsible for providing the location where an update should be stored. Thus, a good algorithm for the block allocator will give good and even wear (copium ðŸ¤ž, but it's possible to do this, unlike other challenges which arise due to the nature of the file system).A disadvantage of CoW file systems is pretty obvious as shown above. A simple update takes too many copies and writes due to updating all the fs objects in the file's path.Another disadvantage is that it needs quite a lot of space to keep copies. If you don't have enough space to update a file (which includes not just the file but ancestors as well), a CoW file system is a bad choice.Another disadvantage can be that CoW file systems require a garbage collector. So, additional memory usage, and processing time. The garbage collector needs to figure out which blocks (old copies) it can safely erase, and which are required in case there is a power failure in the near future. This is not a computationally cheap thing to do.The amount of extra space used by old copies are determined by the aggressiveness of the garbage collector, but no garbage collector should erase copies of the fs objects in the path of a file until it is sure the entire file has been updated.","conclusion#Conclusion":"Too many types of file systems, and too many problems. They try to solve some problem, but end up creating another, or solving it partially. Mnemofs is heavily inspired from littlefs, which itself tries to take best of both CoW and journalling file systems and combines it with some ingenious problem solving.We'll look into mnemofs, and specifically, its LRU in the next part."}},"/mnemofs/journal":{"title":"Mnemofs's Journal","data":{"":"Continuing a reoccuring theme, to understand this, we need to understand something else before. Since journal stores updated information about a file or a directory, first we need to look into how mnemofs stores a file or a directory.","count-trailing-zero-ctz#Count Trailing Zero (CTZ)":"On a seemingly unrelated note, have you heard of the CTZ operation? It, as its name suggests, counts the number of trailing zeroes in the binary representation of a number.eg. 1860 is 11101000100 in binary, and there are 2 trailing zeroes in it. Thus, ctz(1860) == 2.GNU compilers provide a __builtin_ctz(x) for this, and in C 23, it's become a part of the official standard. Most CPU architectures support this instruction.","ctz-skip-list#CTZ Skip List":"A skip list is a modified singly linked list that, instead of containing one pointer per node to point to the next node, contain more pointers in addition to the original pointer. Also, as shown by littlefs (who pioneered the CTZ skip list data structure), Copy-On-Write benefits from a backward linked list (or a backward skip list) than a forward skip list.Skip lists prefer to keep the number of pointers per node as random to lower the cost of insertion and deletion. However, a Copy-On-Write file system has no need of \"insertion\" and \"deletion\". All of the operations are of the form of \"appending\" and all modifications requested are done in memory. We'll discuss how a CTZ skip list works in mnemofs, but first we need to know what is the structure of a CTZ skip list.In CTZ skip lists, each CTZ skip list block at index x has ctz(x) + 1 number of pointers (0 for 0th CTZ skip list block). Each CTZ skip list block has pointer to (x - 2^i)th CTZ skip list block for every i such that x is divisible by 2^i. For example, CTZ skip list block with index 6 will have pointers to 5th and 4th CTZ skip list blocks, while CTZ skip list block with index 8 will have pointers to 7th, 6th, 4th and 0th CTZ skip list blocks.In mnemofs, each CTZ skip list block takes exactly one page of space.Since it's possible to iterate to any CTZ skip list block from the very last CTZ skip list block, only the page number and index of the last CTZ skip list block is stored, along with the size of the file. Mnemofs uses CTZ skip lists like its creator, littefs, does. However, mnemofs uses it to represent files and directories.","travel-and-offset#Travel and Offset":"We'll use the word \"offset\" to refer to \"data offset\", which is the offset into the actual data contained in the CTZ skip list, which doesn't include the pointers.Conversion of the offset into its CTZ skip list block index and page offset can be done through the derivations done by littlefs.Travel from one CTZ skip list block to the other can be done using a greedy approach that utilizes the fact that the powers of 2 that change from one CTZ skip list block to the other while travelling first monotonically increase, and then, monotonically decrease. The graph might be discontinuous, but that's not an issue. It's actually easier to understand by reading the code in this case.","journal-logs#Journal Logs":"Back to the journal, now, once the updated information of the CTZ skip list is received, it is logged to the journal along with a CTZ skip list representation of the path of the FS object. This log is followed by a checksum to make sure that the entire log was written correctly to the flash.","structure#Structure":"The journal consists of blocks from the NAND flash. The last two blocks allocated for the journal are reserved for the master node, and are called master blocks (more on that later). The first block starts with an 8 byte magic sequence, followed by the number of blocks (all n + 2 blocks) allocated to the journal, and then an array that contains the block numbers of all the blocks allocated to the journal.This is like a modified version of a singly linked list. A traidional singly linked list design was not used to allow the mount process to quickly find the master node once the journal was found (more on that later), and it reduces space, as a traidional design would require the last page of every block in the journal to be reserved specifically for storing the block number of the next block.The first n blocks (out of the n + 2 blocks) store the logs, and once full, the journal gets flushed, but more on that later.","conclusion#Conclusion":"So, this is how the journal works, and the \"more on that later\" parts will be explained in the next section that discusses the master node of mnemofs."}},"/mnemofs/lru":{"title":"Mnemofs Least Recently Used (LRU) Cache","data":{"":"Before diving into the LRU Cache, called LRU for short, we need to look into a data structure that every one seems to know, but with a slightly different flavor.","kernel-linked-lists#Kernel Linked Lists":"Linux, and NuttX (among others) have this very special flavor of linked lists that seem to make it just right, called a kernel linked list, or simply, a kernel list. This is a circular doubly linked list which can be used to store any data type.Below is a traditional circular doubly linked list.And the below is a kernel list.The type struct list_head contains only pointers to other struct list_head. It doesn't care about the structure it is part of. This can allow traversal, and conversion of any knd of structure\nwe want into a list. The question is, if we have a pointer, how do we get the original structure back?The answer to that is pretty simple and brilliant. Pointer offsets. If your struct struct my_struct contains a member struct list_head list, then, let's say, the offset of list from the start of the struct is off, then if we have an address x pointing to list, we can get its parent by just doing x - off. off will always remain constant for a given struct, and there are utilities provided to calculate them. In fact, the list utilities don't require you to even have to think too much about how this works.","lru-structure#LRU structure":"Back to the LRU. The LRU is in-memory, and its main purpose is to reduce the wear of your storage device. The way it does is by bunching some changes to the same file, and then writing them all in one go. LRU bases itself off of the structure of a traditional LRU design, and so, like any good LRU, it needs a doubly linked list implementation. We'll go a bit further than that and use kernel lists.The LRU in mnemofs is a kernel list of nodes. Each node represents a file or a directory. Each node contains deltas, which are basically the updates a user wants. The deltas are arranged in a kernel list for code reduction, however, they may use something as simple as a singly linked list. Deltas are of two types: either put x bytes at an offset off by replacing at maximum x bytes (less than x bytes at the end of a file), or delete x bytes from offset off.When a new node is to be inserted, and if the LRU is full, the last node (tail of the list) is popped off, and all the deltas in it are written to the flash. This is called the flush operation. A flush operation may happen implicitly as explained, or explicitly in cases like where a file is closed.When the deltas are written to the flash in an Copy-On-Write (CoW) manner, the new location and size is changed and the journal comes into play here. This need to be updated in the parent as well. Thus the parent goes through this same procedure for updates as well.CoW file systems face a very common problem of cascading or bubbling up of updates. If a file is updated, its location changes. Thus the parent needs to be updated, and its location is changed as well, and so on this rises up the file system till the root is updated. However, unlike most CoW systems, the LRU does not let the updates bubble up into the file system immediately.The LRU isn't a cache in a strict sense, as the original data still needs to be read from the flash before applying the changes to the LRU, and thus, does not \"save\" time like usual caches. However, the main purpose it has is to batch updates together to reduce the number of times the file is updated in the flash. The size of the LRU is configurable during compile time, thus giving control over the RAM vs wear tradeoff. Also, if someone wants to apply the updates to the flash as soon as possible, then the LRU size should be kept to a minimum.Unlike traditional caches, we're quite happy to use the fact that while using caches, the original store is not up to date with the in-memory store as this stops (or rather, staggers) the upward propagation of the updates to the root. My apologies to all those great people who have worked to solve this issue in other areas of computing.\nPower loss will cause all changes in the LRU to be lost. In fact, it's the only bunch of updates that will be lost. The updates in the journal will remain, and the file system will be in a recoverable state at all times.The smaller the LRU is configured, the lesser you will lose after a power loss.","applying-updates#Applying updates":"When it's time to update convert the deltas into actual data on the flash, it's pretty complicated. The first task is to determine what doesn't need to change. If a file takes up blocks a, b, c, d and if the block c has some deltas, then the new CTZ list has to be like a, b, x, y due to CoW. Thus, the \"prefix\" needs to be determined.After that, the","conclusion#Conclusion":"After the LRU, it's turn to look at the journal in mnemofs."}},"/mnemofs/mideval":{"title":"GSoC '24 @ Apache NuttX, Mid Term Evaluation Blog","data":{}},"/mnemofs/mn":{"title":"Mnemofs's Master Node","data":{"":"The master node in mnemofs is the entity that can be used to get the location of the root of the file system.When the journal is flushed (or, rather, the first n blocks are flushed), then all of the changes in the journal are applied to the entire file system in a true CoW way, and this finally cascades to the root of the file system, till the root is updated.At this state, both the old data and the new data exist simultaneously, and the new location of the root is recorded in a new master node, and written to the flash. Once the new master node is written to the flash, only the the entire file system's \"base state\" gets updated, and now there is no risk of losing changes or leaving the device in an unrecoverable state (at least, till new updates come in). The block allocator ensures that both the data exist together, and that old pages are not erased, till the new master node is not written, which can allow for rollback of changes.The master blocks are duplicates of each other, and contain multiple instances of the master node. Master node takes no more than a single page, and contains information about the root, in a way similar to a parent directory. The master blocks contain various \"revisions\" (or instances) of the master blocks. It's important to note that only the latest master block is a \"valid\" master block. This means that all older master blocks do not point to a valid file system.","journal-flush#Journal Flush":"The first n blocks of the journal will be erased much more often than the master blocks, due to the design of the file system. This means that the journal needs to move for every time it's flushed. However, not all of the journal needs to follow this. The master blocks will move only when they are full, which happens after each integral number of journal flushes. Thus, there are some times when the journal moves alone, while at others, the journal moves along with the msater blocks.","conclusion#Conclusion":"Here you saw how the journal of mnemofs works. Now, moving on to the block allocator."}},"/mnemofs/mnemofs":{"title":"mnemofs","data":{"":"Mnemofs is a NAND Flash File System designed and built for NuttX as part of the Google Summer of Code 2024 program. Mnemofs is designed to be a middle ground between flash storage consumption, memory consumption, wear and speed. It sacrifices a little bit of everything, and ends up being acceptably good in all of them, instead of sacrificing multiple aspects, and being good in one.Mnemofs consists of several components, and thus, we approach it one by one. Baby steps.Here are the blogs that cover this in depth:\nIntroduction Part I\nIntroduction Part II\nIntroduction Part II\nMnemofs\nMnemofs LRU\nMnemofs Journal\nMnemofs Master Node\nMnemofs Block Allocator\nGSoC '24 @ Apache NuttX, Mid Term Evaluation Blogs"}}}