{"/coming_soon":{"title":"Coming Soon","data":{}},"/":{"title":"resyfer's Dev Blog","data":{"":"Hi, I am Saurav Pal, and I code under the name resyfer. Here's a blog to share my two cents on stuff.","about-me#About Me":"I'm an enthusiastic developer who's open to learning anything and everything related to operating systems, embedded systems, and networking, and I like open source.I am a Google Summer of Code 2024 (GSoC '24') contributor to Apache organization in which I am working on designing and implementing mnemofs, which is a filesystem for NAND Flashes for the Apache NuttX Real Time Operating System (RTOS).I'm also currently an upcoming Associate Software Engineer at Oracle.I was also a GSoC '23 contributor to PostgreSQL as well, working on improving PostgreSQL version support for pgexporter.I am currently a fresh graduate from the National Institute of Technology, Silchar, which is one of the Institutions of National Importance in India, in which I was pursing a Bachelor of Technology (B. Tech.) in Computer Science and Engineering (CSE) for 4 years (from Nov 2020 - May 2024).","blogs#Blogs":"Here are my blogs.\nmnemofs"}},"/mnemofs/ba":{"title":"Mnemofs's Block Allocator","data":{"":"The block allocator of mnemofs is inspired by littlefs a lot, but it differs a lot. Since NAND flashes are a common device used for sensors, the allocation of blocks or pages needs to be fast. This means shifting the RAM vs. speed tradeoff a little bit so that it consumes more RAM.The block allocator wants to be fair and spread wear as evenly as possible. So, like littlefs's block allocator, this starts with a random block and keeps assigning pages. Now, in mnemofs, pages and blocks can both be expected from the block allocator. In this case, the block allocator skips pages until the start of the next nearest block, and allocates from there. Since most block allocations are done by internal data structures, and they happen in bulk, skipped pages are not too much in count. Contiguous allocation may happen, but it may not as well.The block allocator contains a bitmap of each page to denote if it's being used or not. The block allocator also contains an array of counters for each block in the device. These counters count the number of pages inside that block that want to be deleted. If all of the pages inside a block want to be erased, then the next time the block allocator comes across this block (or any page in it), it will immediately erase the block and then allocate blocks or pages.","conclusion#Conclusion":"So, all in all, these are the ins and outs of mnemofs. Is it a weird place to end the series? There are follow-up blogs of my journey in designing and implementing this file system!"}},"/mnemofs/intro_p1":{"title":"Introduction Part I | mnemofs","data":{"":"So, designing and implementing a file system...it's a daunting task, and very overwhelming as well. BUUUUUT here's me delving into the concepts to the best of my knowledge. Grab a drink or some popcorn as it's going to take a while.","storage#Non-Volatile Storage":"Non-volatile storages like hard disk drives (HDDs), Solid State Drives (SSDs), Compact Disks (CDs), Pen Drives, NAND Flash Devices, etc. are basically entire arrays that you can use to store anything.What makes it different from arrays in your Random Access Memory (RAM)? Many things. But the most important of them is that it is non-volatile. This means that if something is stored in it, it will remain stored in it even if power is no longer supplied to the device. As an additional bonus, as far as storage mediums that exist till date go, if you store even a byte at a location, say, x, then it will be at x unless deleted explicitly (or some unexpected event can modify your storage, but more about it later!).Another difference is that, usually with an operating system (OS) running on your device, in RAM, you can not really specify where you want data to be stored in terms of absolute position in the device. If you try to write, say, a byte, at position x, it needs to be within the allowed locations by the kernel of the OS. Without getting too much into it here, as it is out of scope, if you do not write within the confines specified, you get hit with bad karma in the form of Segmentation fault (core dumped).\nThere might be people trying to say you cannot do that to a non-volatile device running on an OS as a user either blah blah, while others will counter it with their own written kernel module to do it but those are technicalities, and it will only result in confusion.\nThus, having established that they are different, we'll be calling volatile memory (primary memory) like RAM as memory and non-volatile memory (or secondary memory) like HDDs, etc. as storage. Storage is slower than memory, but it is also cheaper per byte. Adding the advantage of the non-volatile nature of storage shows you why it's such a popular medium of storing things.In and of itself, storage seems like a very good thing, but it is like a fox in sheep's clothing. It can go horribly wrong if not used properly. In fact, the false sense of security of having your data persist can make the shock even bigger if you can not access your data for whatever reason due to improper use.","fs#File Systems":"","fs-need#Why are they needed?":"A thing to note is that a user would be better off printing out everything compared to managing the data on a storage device manually. Why? Here's an example to help you grasp that situation:Let's say there are 16 Bytes (16 B) of storage on a device, and you want to store 4 pieces of information, each of 4 B. If we split it into blocks of 1 B each, we can give 4 consecutive blocks to each piece and write it down somewhere, say on a piece of paper, where each piece is. Say, after some time, the 4th piece is no longer required, but your 2nd piece also suddenly needs 3 B instead of the original 4. So you will clear the 4th piece's data from the device and clear the last block given to your 2nd piece, so it only has 3 B. Now the 8th, 13th, 14th, 15th, and 16th blocks are clear of data. Now, if another new piece (5th piece) wants 2 B of storage, you give 8th and 13th place to it, and so on and on.\nIf you got confused while reading the example, that just proves my point.\nThis is very confusing to think about and, more importantly, difficult to remember. And here we are, barely talking about 16 B of information space and 4 pieces of information. Reality is harsher. We need to store SOOOOO many things. Your high-definition (HD) photos, your movies, your applications, your games, and so on. A normal user just wants to store things without pain. The above-mentioned method is very much an alternative definition of pain.\nPain always exists.\nIf you can't feel it,\nit's probably being shouldered\nby someone else.\n- Me, 2k24\nSo, some developers take that pain away from you onto their shoulders, and bring you file systems which are programs that manage your storage without you having to think about them too much.","file-system-components#File System components":"File systems have a lot of components. Like a car, some of them are visible to a user, and they are aware of their presence, while the others are kept under the hood. These components are called file system objects (FS objects).Also like the parts of a car, these fs objects have different names depending on whom you ask. We'll stick to Linux (/ Unix / POSIX) terminology here, but at the end, it is just a name.We'll go through some of the very common ones below, while we keep others for later exploration as we delve deeper into file systems.","file#File":"In a traditional sense, any collective piece of information that is part of one entity is one file. Like a photo. One entire entity. It is a very loose definition. What if you cropped an image into 4 pieces and kept them separately? You do you; no judgment.Take it like a collection of information that you want to keep together, such that programs can work on the entire thing together. An image viewer can show you an entire image in one go if you keep all of it together. A video player can play your entire movie in one go if kept in one piece. And so on.They are analogous to a single paper or multiple papers that are kept together in something called a file (usually seen in the old photos of offices from a time close to the ice age) as they belong to the same topic.","directory#Folders / Directories / Drawers":"Again, work terminology. They are called many different things. We'll stick to directories, as Linux calls them. They are just a group of files and/or other directories as well. Why are they even needed? Multiple reasons. The right answer is that it depends on the user. But, here are some of the common reasons:\nOrganization. We like to be viewed as organized intellectual beings. So we organized multiple files in a hierarchical system so that topics become more specific the further down in the hierarchy you go.\nSearching. Suppose you have 500 files in your view. If you want to look for a specific file, you need to go through, in the worst case, all of them. If organized, the search may be way faster, as you would know which directory and the same inside it, and so on.\netc.","symlinks#Symbolic Links (symlinks) / Shortcuts":"Suppose you have a file in a directory that is very deep in a hierarchical system of directories. But suppose there's this folder called \"My Desktop\" in which, if files are kept, they will be shown on...your desktop (ðŸ¤¯). Now, keeping everything on your Desktop would not make sense, and that file is happy where it is. And you're mostly happy with where it is as well. But you need it on your Desktop as well.Well, copy it, duh! But updating one does not update the other. Thus came symlinks. They are a type of file that points to the location of another file. There are two types of symlinks, but...technicalities.Symlinks might not have universal support as they might not be considered essential for the target audience of the OS, but most major general purpose OSes do support them.","inodes#Inodes":"Files usually have two types of data associated with them. Their content, and their metadata. Their content is what we usually refer to as files. Their metadata usually refers to information about the file. If you go back to the work terminology, it might be analogous to the information written on the cover of the physical file.These contain information like the file's name, their owner (who created it, or who owns it, etc. in a multi-user environment), their creation date, their last modification date, their last access date, their size, and so on. These are stored as another FS object called an inode.Inodes are very much optional too, and it depends on the file system on how they want to manage the metadata.","conclusion#Conclusion":"So, we glanced over the basic knowledge required to start learning about file systems. The fun and games end here, and in Part II, it's time to dive into the depths of file systems, and file system development."}},"/mnemofs/intro_p2":{"title":"Introduction Part II | mnemofs","data":{"":"We'll dive into an incremental explanation of various concepts involved with file systems and operating systems. It dives into a lot of \"suppose\" and \"what ifs\" to try and get you a sense of the problems that lurk in plain sight. Only by recognizing the existence of these problems can people begin to think about trying to solve them.\nSolutions exist\nonly after problems\nare identified.\n- Me, 2k24","fs-os#File Systems and Operating Systems":"If you develop your own OS, and if you develop your own file system (FS), and if you only want your own FS in your OS, then you can develop them pretty much however you like them. Your OS can change to suit your FS needs, or vice versa. There is no need to be considerate of users or other developers who differ in opinion. Make your own OS-FS pair if you are so much better! There is no need to be the most efficient! If it gets the job done, it is good enough!This \"true power\" is probably what was felt by the developers of DOS decades ago.The world has changed since then. For better or worse, it has become too complex and demanding for one solution to be efficient or even sufficient for all needs out there. If you're a FS developer, not only is there someone out there who might do it better, but you also need to tailor your FS to support a very specific subset of storage devices and technologies to be efficient in that subset of storage technologies.","vfs#Virtual File Systems (VFS)":"","vfs-why#Why?":"Let's say I am an OS developer, and my under-development OS has this file system that I have named abcdfs, and I have to make my OS interact with the FS using the methods that my FS exposes, like:\nint abcdfs_open_file_from_path(char *path);\nint abcdfs_close(char *path);\nint (char *path);\nIn such a world, if I am developing my own OS-FS pair, my FS doesn't really have a hard boundary on what it can or cannot do. It can try and access the internet, send an email, run your game, etc.Can you see the problem? The implementation of the communication between an OS and FS would change depending on the FS. No standardization, a free but chaotic mess.Also, early on, in the 1980s, with the rise of a lot of FSs that seemed to be more or less similar to each other, there was a rise of the opinion that an OS should allow users to choose their preferred file system.A nightmare if you combine the problems.So the OS developers put some restrictions on the FS developers in the form of standardization. Wherever there is standardization, there are restrictions, but also flexibility in choosing solutions.","vfs-what#What?":"A Virtual File System (vfs) is basically a simple interface that the OS demands an FS implement. This allows the OS to use multiple file systems at once, and execute the necessary ones.For example, if an OS defines its interface to be like this:\nstruct vfs_ops {\n    char (*open)(char *),\n    char (*close)(char *),\n};\nthen it means that it wants every file system to have at least one open and one close function. The file systems can then expose it like:\nstruct vfs_ops abcdfs_vfs_ops = {\n    .open = abcdfs_open_file_from_path,\n    .close = abcdfs_close,\n};\nThis interface is what the OS will use to interact with the file system. If a user wants to open a file, then the OS can expose a system call like int open(char *path) for the user. The internal implementation can be over-simplified to this:\nint open(char *path)\n{\n    // headache stuff\n    struct vfs_ops ops = get_vfs_ops_from_path(path); // ops == abcdfs_vfs_ops\n    ops->open(path);\n    // even more headache stuff\n}\nThis has some obvious advantages:\nThe user does not need to care about the underlying FS being used.\nThe FS developer does not need to develop their own OS.\nThe FS developer just needs to modify their existing FS to include the interface of the OS they want to run their FS on.","drivers-and-operating-systems#Drivers and Operating Systems":"Without diving into too much detail, a lot of OSes also have an interface similar to a VFS for device drivers, which includes drivers for storage devices.Now the FS developers do not have to explicitly support a very specific storage device from a very specific manufacturer, but can become more generalized. Instead of making a FS that supports only Samsung 1TiB SSDs, I can move on to making an FS that instead, say, supports only SSDs, leaving the implementation details to the drivers provided by the manufacturers.Again, redistribution of pain.So, in a way similar to the communication between OS and FS, the FS can now interact with the storage device through the OS using their respective interfaces, like:\nint abcdfs_open_from_path(char *path) {\n    // code\n    struct device dev = get_device_from_path(path);\n    if(device_type(dev) != STORAGE_DEVICE) {\n        return -EINVAL;\n    }\n    struct storage_driver_ops ops = get_storage_driver_ops_from_device(dev);\n    ops->read_at_offset_in_bytes(offset);\n    // more code\n}","entire-flow#Entire Flow":"Let's go over the entire flow.A storage device is connected to your computer or machine, and the CPU can communicate with it using the driver it provides (not going into too many details of this part here as it deals with hardware I/O).Your OS will have that one FS it supports internally. It is upto the OS developer, and can be anything...rootfs, fat32, etc. This maintains a global hierarchical system from the root /. Due to the whole \"everything in Unix is a file\" thing, your device may be visible as a device in with a path like /dev/my_device. Again, it exploits the whole VFS-is-just-an-interface thing. As long as it is follows the interface, it can still do pretty much anything on your OS, given enough permissions. A free, but not chaotic, mess ðŸ¥³.If you want to use this device with a file system, you need to mount it. So, this storage is then mounted at a certain location in your computer's location, say, /hi. This process involves mentioning the file system that you want to mount this device with, say abcdfs. This creates a directory /hi, which will now serve as the root for this storage device. The metadata about the mount process is stored by the OS (where it is stored is implementation-dependent).Now, when a user wants to, say, create a file /hi/my_file3, they will call the required system call (syscall) that the OS provides for creating a file at a given path. Inside this syscall, the OS will try to create a file /my_file3 on the storage device mounted at /hi. It will get the VFS operations of the file system that was used to mount the device at /hi and use the create method exposed by that file system to create that file.The create method exposed by the file system might itself use a write function exposed by the driver of the storage device through the mount point. A mount point contains information about the file system and the device, and thus, both the device's driver operations and the file system's operations can be used through the mount point.This is a very simplified and generalized way in which OS, FS, and devices interact with each other.","conclusion#Conclusion":"Here we learnt how FS interacts with the OS, and in Part III, we'll dive into FS and the various solutions that have emerged through the decades to an apparently \"simple problem.\""}},"/mnemofs/intro_p3":{"title":"Introduction Part III | mnemofs","data":{"":"File systems have existed for almost as long as storage mediums have, which is to say, decades. File systems started out very simple. As the needs of users increased, as operating systems evolved, and as the quirks of storage mediums increased in exchange for providing maximum efficiency under very specific conditions, file systems had to adapt, and they generally became more complex but more specific.The rest of this blog takes heavy inspiration from littlefs's design document, which is a file sytem that deserves to be put in an art gallery.","types-of-file-systems#Types of File Systems":"There have been various file systems with their various quirks, but they can be generally divided into four types.","block-based-file-systems#Block Based File Systems":"These are file systems that represent the used storage space in the form of a tree. They are also the oldest types of file systems out there.They divide the storage into various blocks in which files are stored. Any updates to the files are done in place. Let's say block x contains my file, and if I want to update the file, the same block is rewritten with the new updated file's content.Can you see the problem? Suppose my block is 256 B in size, and that contains a file of size 256 B as well. Say some random bytes, say 13th, 19th, 100th, and 105th bytes, are to be updated to new values by your computer. Let's say the writes can be done at a maximum rate of 1 B at a time. And to add to this, let's say our luck is very bad, and after it has updated the 19th block, there is a power failure, and the write operation stops, and so does your computer. Now there is no information on what changes are remaining, or what has already been written.You get a situation where some of the file is updated while the rest isn't.Nightmare.This is called non-atomicity, or non-resiliency. They are not resilient to power losses as they are not atomic. If an operation is atomic, it means that if it is interrupted in between executions, any changes it makes will be reverted back to the pre-execution state.Some examples of this type of file system are FAT and ext2.Without modifications (as seen later), these file systems do not stand up to modern needs for atomicity. You need your file to be updated, but even the previous state is better than a possibly garbled mess.While this might not seem to be much of an issue for text files, for files encoded using encoding algorithms, this is quite a hell. Depending on the encoding algorithm, it has a very likely outcome of the entire file being corrupted because the decoder can no longer decode the file and hence will give that error.Another disadvantage of such file systems can be that if there is a file that is updated very often compared to others, the location on the storage device, where the file is located, will be used much more than other areas of the device, leading to uneven wear distribution. This may end up causing that particular area of storage device to die before the others.An advantage of such file systems is speed. Due to their simple design, they are very fast.","log-based-file-systems--log-structured-file-systems#Log Based File Systems / Log Structured File Systems":"On the other extreme, there are log based file systems that take atomicity very seriously. Instead of treating the entire storage as an array, they treat it similar to a queue.Any change has a corresponding entry, which is stored in the storage device in a first-in, first-out (FIFO) manner, i.e. each new entry is stored after the end of the last stored entry. To recreate a file, all that is needed is to iterate over all of the entries in a queue. Usually each entry has a checksum of the entry suffixed to it.The checksum is usually a value obtained by hashing an entire log. While reading an entry or log, if the stored checksum does not match the calculated checksum of the log, the log is discarded, as it means that the log was not written properly. If you assume your file system is bug-free, this usually narrows the culprits to power loss.It might seem really great initially, but see the problem? It is very slow. Suppose each read (one bytes) from the device is one instruction (that's quite generous as it's 100x slower than reading a value from RAM! ), and you have a 4 GiB device. This means 4294967296 B, so that many instructions are needed to traverse the entire storage. Ignore any calculations we do with the data; this would take about 1.38 seconds if an Intel i5 10th generation was running on its base frequency of 2.9 GHz (assuming no optimisations). As mentioned, calling it one instruction per read is being generous. This means that every time you open a file, you need to wait 1.38 seconds at the very least. And moreover, the processor used for this calculation is quite a beast in itself.CPUs in embedded systems are slower. STM32F401CCU6 has a CPU with 84 MHz frequency. So, the same operation under the same assumptions would take it 48 seconds. Do remember, this is just 4 GB of storage capacity.Nightmare, but in another direction.An advantage of such file systems is of course, absolute atomicity. But another advantage is wear leveling. Since entries are added in a FIFO manner, it will ensure that any pair of blocks has a maximum wear level difference of 1. The wear levels of the blocks of storage would always look like [x+1, x+1,..., x+1, x+1, x, x, x,..., x, x], where the last x+1 wear is the location of the last entry.A disadvantage, apart from being slow, is what happens if the storage becomes full? Changes might be infinite depending on the user, and so will the entries that represent these changes. However, space is not infinite or file system development would not have been so difficult.File systems of this type include JFFS/JFFS2, YAFFS, or SPIFFS.","journalling-file-systems#Journalling File Systems":"The very weird thing is that both of the above types of file systems have mutually exclusive advantages and disadvantages. So naturally, a middle ground approach would either benefit from both, or none.We create a block-based file system, but we also reserve a certain place on the device for the FIFO queue to store logs. We call this FIFO queue the journal or bounded log. Best of both worlds. The block file system contains a sort of \"base\" state, and further changes to it are stored as entries or logs in the journal.To get the updated version of the file, all the FS has to do is take the \"base\" state and iterate over the journal, applying changes to it.Independent of implementation, there is a very strong relationship between storage location and data due to the presence of the journal. This can cause an abnormal increase in the wear of the journal along with the disadvantage of increased wear for a frequently updated file in block based file systems. Another problem is that there are essentially two file systems running in parallel, and both code complexity and execution may increase.Depending on the implementation, this may have some additional problems. The file system may decide to commit logs to the file system when the journal is full. This means that the \"base\" state needs to be updated, and thus, the journal is emptied. The changes may be committed one by one, but power loss during committing the journal may cause garbled data as well.This is the most popular category of file systems, and this category includes Linux's most popular file system ext4, and Window's most popular (actually you don't have a choice, as far as daily files go) file system NTFS.","copy-on-write-file-systems#Copy On Write File Systems":"A file system category based on an entirely new way of dealing with this problem is Copy-On-Write (CoW) file systems.What does CoW mean? You want to update a value? Copy the entire thing and write on that copy. It's similar to how some developers program before learning about the existence of version control.Suppose a block (I'll give it a name x) is at a location p. If you want to update it, you will read the entire block in memory, update the necessary parts in memory, and write the updated block to a new location q.This sounds simple, but it's a bit more complicated than that when you dive into the nuances of it.So, files and directories exist in all these types of file systems, which include CoW file systems. This means that there is a hierarchical ordering of files and directories. A directory is a collection of files, so under working CoW file system implementations, a directory has some information about the location of a file.Let's update our file! Let's assume our file is just one block in size for simplicity. So our file got updated, and its location shifted from p to q. But the directory that contains our file still points to p!!! So, we need to update our directory! But now, the parent directory of this directory faces a similar problem. This continues to propagate upward until it reaches the root of the entire file system's tree. The root has no parent, so updating it updates the tree. BUT, unless your root is confined to some specific places in the device, you would need to store where the root is located as well, and this update problem again continues until it finally reaches something that either has a single fixed location (in which case it doesn't follow CoW as it would need to be updated without changing location) or it has multiple fixed locations, in which case the file system has to figure out which one of the locations contains the most recent update.Yep...A lot of problems, and a lot of headache.An advantage of CoW file systems is that there are two copies of the block. Old and updated. If the update was not successful, the old version would be used.The wear leveling here depends on the block allocator, which is responsible for providing the location where an update should be stored. Thus, a good algorithm for the block allocator will give good and even wear (copium ðŸ¤ž, but it's possible to do this, unlike other challenges that arise due to the nature of the file system).A disadvantage of CoW file systems is pretty obvious, as shown above. A simple update takes too many copies and writes due to updating all the FS objects in the file's path.Another disadvantage is that it takes up quite a lot of space to keep copies. If you don't have enough space to update a file (which includes not just the file but ancestors as well), a CoW file system is a bad choice.Another disadvantage can be that CoW file systems require a garbage collector. So, additional memory usage, and processing time. The garbage collector needs to figure out which blocks (old copies) it can safely erase, and which are required in case there is a power failure in the near future. This is not a computationally cheap thing to do.The amount of extra space used by old copies is determined by the aggressiveness of the garbage collector, but no garbage collector should erase copies of the FS objects in the path of a file until it is sure the entire file has been updated.","conclusion#Conclusion":"There are too many types of file systems, and there are too many problems. They try to solve some problem but end up creating another or solving it partially. Mnemofs is heavily inspired by littlefs, which itself tries to take the best of both CoW and journaling file systems and combine them with some ingenious problem solving.We'll look into mnemofs, and specifically, its LRU in the next part."}},"/mnemofs/journal":{"title":"Mnemofs's Journal","data":{"":"Continuing a reoccurring theme, to understand this, we need to understand something else. Since the journal stores updated information about a file or a directory, first we need to look into how mnemofs stores a file or a directory.","count-trailing-zero-ctz#Count Trailing Zero (CTZ)":"On a seemingly unrelated note, have you heard of the CTZ operation? It, as its name suggests, counts the number of trailing zeros in the binary representation of a number.e.g., 1860 is 11101000100 in binary, and there are 2 trailing zeroes in it. Thus, ctz(1860) == 2.GNU compilers provide a __builtin_ctz(x) for this, and in C 23, it's become a part of the official standard. Most CPU architectures support this instruction.","ctz-skip-list#CTZ Skip List":"A skip list is a modified singly linked list that, instead of containing one pointer per node to point to the next node, contains more pointers in addition to the original pointer. Also, as shown by littlefs (who pioneered the CTZ skip list data structure), Copy-On-Write benefits more from a backward linked list (or a backward skip list) than a forward skip list.Skip lists prefer to keep the number of pointers per node at random to lower the cost of insertion and deletion. However, a Copy-On-Write file system has no need for \"insertion\" and \"deletion.\" All of the operations are in the form of \"appending\" and all modifications requested are done in memory. We'll discuss how a CTZ skip list works in mnemofs, but first we need to know what the structure of a CTZ skip list is.In CTZ skip lists, each CTZ skip list block at index x has ctz(x) + 1 number of pointers (0 for the 0th CTZ skip list block). Each CTZ skip list block has pointer to the (x - 2^i)th CTZ skip list block for every i such that x is divisible by 2^i. For example, a CTZ skip list block with index 6 will have pointers to 5th and 4th CTZ skip list blocks, while a CTZ skip list block with index 8 will have pointers to 7th, 6th, 4th, and 0th CTZ skip list blocks.In mnemofs, each CTZ skip list block takes exactly one page of space.Since it's possible to iterate over a CTZ skip list to reach any CTZ skip list block from the very last CTZ skip list block, only the page number and index of the last CTZ skip list block are stored, along with the size of the file. Mnemofs uses CTZ skip lists like its creator, littefs, does. However, mnemofs uses it to represent files and directories.","travel-and-offset#Travel and Offset":"We'll use the word \"offset\" to refer to \"data offset,\" which is the offset into the actual data contained in the CTZ skip list, which doesn't include the pointers.Conversion of the offset into its CTZ skip list block index and page offset can be done through the derivations done by littlefs.Travel from one CTZ skip list block to the other can be done using a greedy approach that utilizes the fact that the powers of 2 that change from one CTZ skip list block to the other while traveling first monotonically increase and then monotonically decrease. The graph might be discontinuous, but that's not an issue. It's actually easier to understand by reading the code in this case.","journal-logs#Journal Logs":"Back to the journal. Now, once the updated information on the CTZ skip list is received, it is logged to the journal along with a CTZ skip list representation of the path of the FS object. This log is followed by a checksum to make sure that the entire log was written correctly to the flash.","structure#Structure":"The journal consists of blocks from the NAND flash. The last two blocks allocated for the journal are reserved for the master node and are called master blocks (more on that later). The first block starts with an 8-byte magic sequence, followed by the number of blocks (all n + 2 blocks) allocated to the journal, and then an array that contains the block numbers of all the blocks allocated to the journal.This is like a modified version of a singly linked list. A traditional singly linked list design was not used to allow the mount process to quickly find the master node once the journal was found (more on that later), and it reduces space, as a traditional design would require the last page of every block in the journal to be reserved specifically for storing the block number of the next block.The first n blocks (out of the n + 2 blocks) store the logs, and once full, the journal gets flushed, but more on that later.","conclusion#Conclusion":"So, this is how the journal works, and the \"more on that later\" parts will be explained in the next section that discusses the master node of mnemofs."}},"/mnemofs/lru":{"title":"Mnemofs Least Recently Used (LRU) Cache","data":{"":"Before diving into the LRU Cache, called LRU for short, we need to look into a data structure that every one seems to know, but with a slightly different flavor.","kernel-linked-lists#Kernel Linked Lists":"Linux, and NuttX (among others) have this very special flavor of linked lists that seem to make it just right, called a kernel linked list, or simply, a kernel list. This is a circular doubly linked list which can be used to store any data type.Below is a traditional circular doubly linked list.And the below is a kernel list.The type struct list_head contains only pointers to other struct list_head. It doesn't care about the structure it is part of. This can allow traversal, and conversion of any kind of structure we want into a list. The question is, if we have a pointer, how do we get the original structure back?The answer to that is pretty simple and brilliant. Pointer offsets. If your struct struct my_struct contains a member struct list_head list, then, let's say, the offset of list from the start of the struct is off, then if we have an address x pointing to list, we can get its parent by just doing x - off. off will always remain constant for a given struct, and there are utilities provided to calculate them. In fact, the list utilities don't require you to even have to think too much about how this works.","lru-structure#LRU structure":"Back to the LRU. The LRU is in-memory, and its main purpose is to reduce the wear of your storage device. The way it does is by bunching some changes to the same file, and then writing them all in one go. LRU bases itself off of the structure of a traditional LRU design, and so, like any good LRU, it needs a doubly linked list implementation. We'll go a bit further than that and use kernel lists.The LRU in mnemofs is a kernel list of nodes. Each node represents a file or a directory. Each node contains deltas, which are basically the updates a user wants. The deltas are arranged in a kernel list for code reduction, however, they may use something as simple as a singly linked list. Deltas are of two types: either put x bytes at an offset off by replacing at maximum x bytes (less than x bytes at the end of a file), or delete x bytes from offset off.When a new node is to be inserted, and if the LRU is full, the last node (tail of the list) is popped off, and all the deltas in it are written to the flash. This is called the flush operation. A flush operation may happen implicitly as explained, or explicitly in cases like where a file is closed.When the deltas are written to the flash in an Copy-On-Write (CoW) manner, the new location and size is changed and the journal comes into play here. This need to be updated in the parent as well. Thus the parent goes through this same procedure for updates as well.CoW file systems face a very common problem of cascading or bubbling up of updates. If a file is updated, its location changes. Thus the parent needs to be updated, and its location is changed as well, and so on this rises up the file system till the root is updated. However, unlike most CoW systems, the LRU does not let the updates bubble up into the file system immediately.The LRU isn't a cache in a strict sense, as the original data still needs to be read from the flash before applying the changes to the LRU, and thus, does not \"save\" time like usual caches. However, the main purpose it has is to batch updates together to reduce the number of times the file is updated in the flash. The size of the LRU is configurable during compile time, thus giving control over the RAM vs wear tradeoff. Also, if someone wants to apply the updates to the flash as soon as possible, then the LRU size should be kept to a minimum.Unlike traditional caches, we're quite happy to use the fact that while using caches, the original store is not up to date with the in-memory store as this stops (or rather, staggers) the upward propagation of the updates to the root. My apologies to all those great people who have worked to solve this issue in other areas of computing.\nPower loss will cause all changes in the LRU to be lost. In fact, it's the only bunch of updates that will be lost. The updates in the journal will remain, and the file system will be in a recoverable state at all times.The smaller the LRU is configured, the lesser you will lose after a power loss.","applying-updates#Applying updates":"When it's time to update convert the deltas into actual data on the flash, it's pretty complicated. The first task is to determine what doesn't need to change. If a file takes up blocks a, b, c, d and if the block c has some deltas, then the new blocks have to be like a, b, x, y due to CoW. Thus, the \"prefix\" needs to be determined, which, in this case, is a, b blocks.Now, we need to apply all updates contained in the deltas over such a file. Due to the limitation of the types of deltas, this gets much simplified into a two pointer method, where the upper limiter of the window shifts inwards by x bytes for every deletion that shifts x bytes inside the window. It might shift x + y bytes in total, where y bytes fall outside the window. The way this two pointer window algorithm works, y will not be lower than the minimum limiter of the window, but rather, can only be higher that the upper limiter. We do not care about this, as the next iteration will take care of it. Also, we increment a deleted bytes counter...say del...by x to denote how many bytes have been deleted before the start of the window. So, then, if we are considering a window x, x + m in the new file, then we need to copy the bytes x + del, x + m + del from the old file. This way del marks the amount of \"compensation\" we need to provide.After the window reaches the end of updates, it's time to save this new                                         location in our journal."}},"/mnemofs/mideval":{"title":"Google Summer of Code '24 @ Apache NuttX, Mid-Term Evaluation Blog","data":{"":"Hi! I am Saurav Pal, a recent graduate, and a regular passionate developer interested in systems and anything low-level. I welcome you to my blog, which covers my contributions and journey as a Google Summer of Code (GSoC) Contributor for Apache NuttX in 2024, from the very start to the end. My project involves designing and implementing mnemofs, a NAND Flash File System for Apache NuttX.","my-contributions#My Contributions":"From before the start of GSoC till the midterm evaluation, I have made a few contributions to NuttX. Keeping the unrelated ones for later, here's my work on mnemofs:\n(#11806) Virtual NAND Flash device for the NuttX Simulator\n(#12658) Mnemofs setup and VFS methods\n(#12661) Mnemofs Block Allocator\n(#12668) Mnemofs parent iterator and path methods\n(#12680) Mnemofs LRU and related CTZ methods\n(#) Mnemofs Journal\n(#) Mnemofs Master Node\nThe file system is in a basic testable state, however, it's riddled with many bugs that need to be solved. The file system needs to undergo thorough testing to ensure it's reliable.","components#Components":"The files for mnemofs code have these meanings:\nmnemofs.c: Entry point to the FS. All the implementations of Virtual File System (VFS) methods are in this file.\nmnemofs_blkalloc.c: Contains Block Allocator. Provides a block or page when needed, also marks used pages, and is responsible for erasing a block when all pages in a block want to be erased.\nmnemofs_fsobj.c: This contains FS Object methods, i.e. path and parent iterator methods to the VFS methods. This abstracts away the file and directory structure in the flash and the management of on-flash data and updation of that with the LRU updates. These interact only with the LRU, which also provides wrappers for underlying methods.\nmnemofs_lru.c: Contains the LRU. This stores updates to a file or directory and bunches up the updates to apply batches of updates in one go, to reduce the number of writes to the flash, or allocations of blocks or pages. This helps in reducing the wear. A higher chance of power loss should be accompanied by a lower configured LRU size. The LRU stays in the memory and interacts with the flash through the CTZ methods. This provides a wrapper over CTZ methods as well, to make it simpler for FS objects to get data with LRU updates applied.\nmnemofs_ctz.c: Contains CTZ methods. These are used to directly write/read CTZ information to/from the flash. This lies as a wrapper over the journal and will update, or be updated from the journal. These abstract away the data portion of the CTZ skip list, and don't let the caller worry about the pointers, and instead treat the data portion across blocks (of varying sizes) as a single unit that can be used with an offset from the start. The journal should not be accessed directly, but instead through this.\nmnemofs_journal.c: Contains the Journal. This contains a lot of logs in a sequential manner. Each log contains a path of the old file in the form of an array of CTZ list information of all the FS objects in the path and the new CTZ list information, and this is appended by a checksum of the log.\nmnemofs_rw.c: Contains the raw read, write, erase, check bad block, and mark bad block methods. These will only be used by the CTZ methods and the journal. These are very simple and are almost just a wrapper over the underlying device driver's exposed methods.\nA detailed blog on the various components can be found here.","key-decisions#Key Decisions":"Some decisions make the implementation differ from the initial design of mnemofs:\nJournal: The journal was initially meant to serve as a circular singly linked list. This would be problematic for a lot of reasons. This would mean that every block would contain the page number, most logically at the very end or the start. Journal blocks are allocated at the start, during the binding phase of a file system. As you can only write to a page once (before erasing), it means that the block numbers have to be written somewhere right at the start. The block numbers are 4 bytes in length, compared to a usual minimum of 512 byte page length. Thus, an entire page per block would be waster for this. So, it was decided that there would be an entire array containing the block numbers in the journal, and it would be preceded by a count of the number of blocks. Since it's a block that has a large size, it's assumed that the array will not extend past a single block. If the array ends in the middle of a page, the rest of the page is left unutilized. Thus, we waste a maximum of slightly less than 1 page worth of storage, compared to the number of blocks worth of pages.\nLRU: The LRU earlier contained a generic interfacing of exchanging x bytes with y bytes. However, applying these commits to an array becomes a nightmare with limited memory usage. Thus, it was narrowed down to two cases... x bytes replaced with x bytes in the updated file (unless it's the end of the file), or x bytes replaced with 0 bytes in the updated file. This made the entire process much simpler, reducing it to be solved with a simple 2-pointer approach.\nWrapper Approach: Almost everything is a wrapper to another layer of methods, even in situations where it simply just calls the corresponding underlying method itself. This was done to reduce cases where one function would, say, update location from the Journal, but one would not, and you need to keep that in your mind, or make mistakes. Here, uniformity helps clear doubts.","my-journey#My Journey":"Disclaimer: Contains a lot of rants and yapping.","pre-gsoc#Pre-GSoC":"","pre-apache-nuttx-period#Pre-Apache NuttX Period":"Before the GSoC period, I was looking into Linux, trying to get started as a contributor. But first, I needed to have a fair understanding of how the whole thing works. Baby steps, you know. Read the famous Robert Love's book, saw a lot of videos, and of course, tried to read the codebase.It was overwhelming, and it was stressful, because... I mean, just look at Linux's codebase and its history. I was also interested in embedded systems, but I was trying to do one thing at a time. While I was going through the codebase, I was hooked on to the history of file systems in the codebase. You could see new file systems appear and some old ones disappear every few versions. Over that, I had heard file systems are the lowest in the whole tree, and are kind of only dependent on the Virtual File System and whatever the storage driver is, and that too that file system developers do not need to worry too much about the storage device (as long as it's of the same technology, but I didn't know this then), or the internals of the OS.So, I felt file systems would be a great starting point. Started reading the codebases of the various file systems. After 3-4 file systems, all of them seemed to swim in front of my eyes and feel the same, and concepts from one FS seemed to merge with concepts from another FS ðŸ« . Still, carried on, as I knew if I tried to read it repeatedly, it would start making sense someday. Every line of C makes sense, as mostly nothing is hidden away, and more so in the case of OS codebases. If it were a higher level language, I would have just probably given up, idk.I wanted to apply for GSoC once again for 2024, and this time, for a large project (I had done a medium project for PostgreSQL in 2023). Around this time, I thought of looking into any OS-related projects that are available, as those align with my interests. Found Apache NuttX. What does NuttX and the project I found have in common with my interests? Everything! File System + Operating System + Embedded Systems. All my boxes ticked, and a match made in heaven, at least from my side ðŸ˜‰.","first-pr#First PR":"So, I decided to look into Apache NuttX and try to understand it. I tried looking into good first issues, but honestly, I was just too optimistic at that point and even the scant few first issues were very advanced for the past me. I could successfully set up NuttX, and build the simulator configuration (I did not have any embedded boards available, but more on that later).I did not wish for hand-holding, as it's often detrimental to the learning process, but I did wish for a nudge in the right direction. And so I contacted Alan who was listed as a mentor to the project on any kind of resources I could gather. He very generously provided some resources and suggested I should look into existing FS codebases (like I had done in Linux).The difference between Linux and NuttX codebases? NuttX codebase looks beautiful. Sure, Linux might look beautiful to the experts, but NuttX looks good even to newbies like me. A lot of the code is properly documented...each function, what it does, and even sometimes, why it exists.I looked at the FAT file system code. In and of itself, the code isn't too complicated. But NuttX's contribution style made a lot of things clear to someone like me who was mostly stumbling around in the dark, trying to make sense of my surroundings. Finally, while reading the codebase, I came across a bug that initially I thought was something that I couldn't understand the meaning of. After asking around on the mailing list, I found out that this bug went unnoticed because the particular way the macro was used in the codebase till then made the expansion correct, even if the macro itself was incorrect, and might cause problems if the usage of the macro was modified. So, I patched it, and made my first Apache NuttX PR!","more-contributions#More contributions":"Alan had mentioned that the FAT file system documentation was a bit lacking and so since I was already looking at the file system, writing it down somewhere would be a good idea, and why not the official documentation :D. So, I researched FAT FS (or, rather VFAT), and alongside reading the code, I wrote down whatever I learned, and finally updated the documentation in my second PR.Now I wanted to move higher in the OS tree. So, I thought about learning about the VFS. Since this VFS shares a lot with Linux's structure, I could apply the VFS resources for Linux here. Along with this, I looked into the codebase and learned more about the VFS. Of course, not all of it, but a lot of it. I updated the documentation for the VFS as my third PR.","gsoc-community-bonding-period#GSoC Community Bonding Period":"This was the community bonding period. Asking questions, talking with the mentor, getting comfortable, exploring the solution proposed, and its feasibility, and exploring any improvements, and not to forget, setting up everything.","nand-flashes#NAND Flashes":"I had an STM32F401 board, but the trouble with it stemmed from the fact that its headers were not soldered. It was quite a trouble getting them soldered, but I had it ready around this time. I had ordered this before my interactions with the NuttX community and Alan, and it was just a buy-whatever-feels-popular-and-cheap. Surprise! It doesn't have a JTAG programmer, and I need to buy one. Another infinite waiting period for that to get delivered to my remote campus.I was sick of this wait and wanted to learn about NAND flashes, and thus, about drivers in NuttX. So my thought process went like this: \"I am already using the simulator. Why not simulate NAND Flash in it? I have already seen QEMU simulate chips. If CPUs, why not peripherals.\" And so, after some headache-filled days, I made my fourth set of PRs (this and this) to add a NAND Flash virtual device as an app in the simulator.This made me learn about both NAND Flashes, as well as a lot of insight into drivers' work in NuttX. Both are very critical for my understanding if I am to develop a file system for NuttX.","setting-up-nuttx-on-my-board#Setting up NuttX on my board":"So, I came back from my campus, to my home, at this time. I have the JTAG programmer and an STM32F401 board. I tried flashing NuttX in it. The LED kept blinking, and Alan said it was because it was crashing (and restarting continuously). It was very weird, because the same NuttX configuration worked for his board, but not mine, which were both STM32F401 boards. To this day, it's a mystery, but it's probably because it's a locally produced cheap copy of the board in my case. Over that, I didn't have a USB-Serial Adapter, so needed to buy one as well.So I got my hands on another board, an STM32F103. This time it was a no-go either. After much looking around, Alan pointed out a frequent issue of the wrong R10 pull-up resistor. While a solution can be found here, I could not modify the board as I was just borrowing it.So, I bought an ESP32-DevKit-V1, and while it's probably a local copy as well, it works with NuttX.","nand-flash#NAND Flash":"Acquiring the NAND flash was a difficult task. None were locally available. Absolutely none. The scant few online were of WSON8 type, which would be hard for me to solder, but they also were of a different voltage specification. I had to import them and bought a W25N from Mouser. Import duty in India is hell, and it cost me 3 times the actual cost of the components.Now, soldering this one was problematic as well. My jumper wires would not have solder stuck to them, nor did I have a breakout board that could fit it. So, I had to improvise. Since the desoldering wick contains copper, which is conductive, I broke a small part of it into 8 small wires (only 8 are required for connection) and then soldered them onto the NAND chip, from these wires, I attached the jumper wires by soldering them to these copper bits.Behold Spider NAND :D.","till-mid-term-evaluation#Till Mid-Term Evaluation":"Till the mid-term evaluation, my PR spree had come to a hiatus. This is because it's very tough to test individual parts of the file system. Almost everything is interdependent, and thus testing one without completing the other gains no knowledge about how it will behave together. Thus, the file system had to be written first. The entirety of it. At least, to a working state.Thus, that's what I did. Wrote it down all.Then, split it into multiple PRs.","apache-workshop#Apache Workshop":"I participated in Apache NuttX International Workshop 2024 as a speaker, talking about my project, and it was amazing. So many opinions, and amazing projects related to NuttX, it was amazing to see them.","mid-term-evaluation-result#Mid-Term Evaluation Result":"Passed :D.","acknowledgement#Acknowledgement":"First up, a thank you to my parents. Words can never be enough for this, and I can never thank them enough for this.Next, a big thank you to Alan, who's my mentor. Spending so much time patiently mentoring and encouraging me, even after multiple blunders along the path...couldn't have asked for more. I have gained a new perspective on a lot of open-source-related things due to the various talks and exposure.Also, a big thank you to Lup and Xiang who have helped me whenever I approached them with my doubts as well.Finally, a thank you to my co-contributor Rushabh Gala and my friends whose support helped me cope with everything."}},"/mnemofs/mn":{"title":"Mnemofs's Master Node","data":{"":"The master node in mnemofs is the entity that can be used to get the location of the root of the file system.When the journal is flushed (or, rather, the first n blocks are flushed), then all of the changes in the journal are applied to the entire file system in a true CoW way, and this finally cascades to the root of the file system, till the root is updated.At this state, both the old data and the new data exist simultaneously, and the new location of the root is recorded in a new master node and written to the flash. Once the new master node is written to the flash, only the entire file system's \"base state\" gets updated, and now there is no risk of losing changes or leaving the device in an unrecoverable state (at least, till new updates come in). The block allocator ensures that both the data exist together and that old pages are not erased until the new master node is written which can allow for rollback of changes.The master blocks are duplicates of each other, and contain multiple instances of the master node. The master node takes no more than a single page and contains information about the root, in a way similar to a parent directory. The master blocks contain various \"revisions\" (or instances) of the master blocks. It's important to note that only the latest master block is a \"valid\" master block. This means that all older master blocks do not point to a valid file system.","journal-flush#Journal Flush":"The master blocks contain multiple master nodes. When the first n blocks of the journal are flushed, a new master node entry is created. Thus, a master block can hold pages_per_block number of master nodes. This also means that the master node does not need to be erased for every journal flush, but rather, in integral multiples of flushes. Thus, there are some times when the journal moves alone, while at others, the journal moves along with the msater blocks, and so, master blocks move much less that the other journal blocks.","conclusion#Conclusion":"Here you saw how the journal of mnemofs works. Now, moving on to the block allocator."}},"/mnemofs/mnemofs":{"title":"mnemofs","data":{"":"Mnemofs is a NAND Flash File System designed and built for NuttX as part of the Google Summer of Code 2024 program. Mnemofs is designed to take a middle ground between flash storage consumption, memory consumption, wear and speed. It sacrifices a little bit of everything, and ends up being acceptably good in all of them, instead of sacrificing multiple aspects, and being good in one.Mnemofs consists of several components, and thus, we approach it one by one. Baby steps.Here are the blogs that cover this in depth:\nIntroduction Part I\nIntroduction Part II\nIntroduction Part II\nMnemofs\nMnemofs LRU\nMnemofs Journal\nMnemofs Master Node\nMnemofs Block Allocator\nGSoC '24 @ Apache NuttX, Mid Term Evaluation Blogs\nThe journey to design and implement this wasn't smooth sailing, and required modifications pretty frequently to either make the FS faster in some respect, or to patch up gaping holes in its logic. All of this will be covered in the mid term and end term blogs for my time as a GSoC Contributor for Apache NuttX."}}}